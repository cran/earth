% earth-notes.tex
%\VignetteIndexEntry{Notes on the earth package}
%\VignettePackage{earth}
\documentclass[12pt,letterpaper]{article}
\usepackage[top=1cm,bottom=1.5cm,left=3cm,right=3cm]{geometry}
\hyphenation{ASM}
\usepackage{amsmath}
\usepackage{url}
% \usepackage[usenames]{color} % for color in text
\usepackage[pdftex]{graphicx}
\usepackage{cite}
\usepackage{verbatim}
\usepackage{subfigure}
\usepackage{sidecap} % for SCfigure
\usepackage{setspace} % for \begin{spacing} in table of contents
\usepackage{enumerate}
\usepackage{pdfsync} % allow pdf viewer to reference text editor (SumartraPdf and epsilon)
\usepackage[pdftex,
  colorlinks,
  urlcolor=nocolor,
  filecolor=nocolor,
  linkcolor=nocolor,
  citecolor=nocolor,
  pdftitle={Notes on the {\tt earth} package},
  pdfauthor={Stephen Milborrow},
  pdfsubject={Notes on the {\tt earth} package},
  pdfauthor={Stephen Milborrow},
  pdfkeywords={MARS, Multivariate adaptive regression splines},
  pdfproducer={pdfLaTeX},
  pagebackref, % creates back references in the bibliography to page numbers
  bookmarks=false,
  bookmarksopen=false]{hyperref}
\usepackage{color}
\definecolor{nocolor}{rgb}{0,0,0}

% for an explanation of below, see www.latex-community.org/forum/viewtopic.php?f=5&t=3722
\renewcommand*{\backref}[1]{}
\renewcommand*{\backrefalt}[4]{
  \ifcase #1 Not cited. \or Cited on page #2. \else Cited on pages #2. \fi
}
% set the float parameters to more-permissive values
% see ``Using Imported Graphics with Latex'' p 55
\setcounter{topnumber}{4}
\setcounter{bottomnumber}{4}
\setcounter{totalnumber}{10}
\renewcommand{\textfraction}{0.01}
\renewcommand{\topfraction}{0.99}
\renewcommand{\bottomfraction}{0.7}
\renewcommand{\floatpagefraction}{0.66}

\begin{document}

\title{Notes on the \texttt{earth} package}
\author{\\
Stephen Milborrow}
\maketitle
\setcounter{tocdepth}{3}
\tableofcontents

\parindent 0ex
\parskip 2ex

\clearpage
\section{Introduction}

This document is a set of notes to accompany the \texttt{earth}
package~\cite{earthpackage}.
It comes with the \texttt{earth} package, or can be downloaded from
\url{www.milbo.org/doc/earth-notes.pdf}.

Most users will find it unnecessary to read this entire document.
Just read the parts you need and skim the rest.
Most of this text was originally written in response to email from users.

The R \texttt{earth} package builds regression models using the
techniques in Friedman's papers ``Multivariate Adaptive
Regression Splines''~\cite{Friedman}
and ``Fast MARS''~\cite{FriedmanFast}.
The term ``MARS'' is copyrighted and thus not used
in the name of the package.  The package can be downloaded from
\url{cran.r-project.org/web/packages/earth/index.html}.

\clearpage
\section{Overview}

Earth has numerous arguments, but many users will find
that the following are all they need:
\vspace{-4mm}
\begin{description}

\item[\hspace*{5mm}\texttt{formula}, \texttt{data}] Familiar from \texttt{lm}.\vspace{-2mm}

\item[\hspace*{5mm}\texttt{x}, \texttt{y}\hspace*{19.5mm}] Alternative to the formula interface.\vspace{-2mm}

\item[\hspace*{5mm}\texttt{degree}\hspace*{14mm}] The maximum degree of interaction.
Default is \texttt{1}, use \texttt{2} for first-
\hspace*{24.5mm}order interactions of the hinge functions.\vspace{-2mm}

\item[\hspace*{5mm}\texttt{nk}\hspace*{23mm}] The maximum number of MARS terms.
The default is determined \hspace*{23mm}
semi-automatically from the number of predictors in \texttt{x}, but some-
\hspace*{25mm}times needs adjusting.\vspace{-2mm}

\item[\hspace*{5mm}\texttt{trace}\hspace*{16.5mm}] Trace operation.
Use \texttt{trace=1} to see why the forward pass termi-
\hspace*{24.5mm}nated, and if necessary increase \texttt{nk}.

\end{description}
% \vspace{-3mm}

% For GLM models, use the \texttt{glm} argument (Section~\ref{sec-glm}).

% For cross validation, use the \texttt{nfold} argument (Section~\ref{sec-cv-and-plot}).

\subsection{References}

The Wikipedia article~\cite{Wikipedia} is recommended for an
elementary introduction to MARS
\url{http://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines}.

The primary references are the Friedman MARS papers~\cite{Friedman,FriedmanFast}.
Readers may find the MARS section in Hastie, Tibshirani, and
Friedman~\cite{ESL} a more accessible introduction.

Faraway~\cite{Faraway} takes a hands-on approach, using the ozone data
to compare \texttt{mda::mars} with other techniques. (If you use Faraway's
examples with \texttt{earth} instead of \texttt{mars}, use
\texttt{\$bx} instead of \texttt{\$x}, and check
out the book's errata.)

Friedman and Silverman~\cite{FriedmanSilverman} is recommended
background reading for the MARS paper.

Earth's pruning pass uses code from the \texttt{leaps}~\cite{leaps} package
which is based on techniques in Miller~\cite{Miller}.

\subsection{Other implementations}

Given the same data, \texttt{earth} models are similar to but not
identical to models built by other MARS implementations.
The differences stem from the forward pass where small
implementation differences (or perturbations of the input data) can cause
somewhat different selection of terms and knots (although similar GRSq's).
The backward passes give identical or near identical results,
given the same forward pass results.

The source code of \texttt{earth} is derived from the function \texttt{mars}
in the \texttt{mda} package written
by Trevor Hastie and Robert Tibshirani~\cite{mda}.
See also the function \texttt{mars.to.earth} (in the \texttt{earth} package).

The term ``MARS'' is trademarked and licensed exclusively to
Salford Systems \url{www.salfordsystems.com}.
Their implementation uses an engine written by Friedman.
It has a graphical user interface and
includes some features not in \texttt{earth}.
Salford Systems has a reputation for excellent customer support.

StatSoft also have an implementation which they call ``MARSplines''
\url{www.statsoft.com/textbook/stmars.html}.

\subsection{Limitations}

The following aspects of MARS
are mentioned in Friedman's papers but not implemented in \texttt{earth}:
\vspace{-4mm}
\begin{enumerate}[(i)]
\item Piecewise cubic models (to smooth out sharpness at the hinges).\vspace{-3mm}
\item Model slicing (the \texttt{plotmo} function goes part way).\vspace{-3mm}
\item Handling missing values.\vspace{-3mm}
\item Automatic grouping of categorical predictors into subsets.\vspace{-3mm}
\item The $h$ parameter of Fast MARS.
\end{enumerate}

\subsection{The forward pass}
\label{sec-the-forward-pass}

Understanding the details of the forward and pruning passes
will help you understand \texttt{earth}'s return value and
the admittedly large number of arguments.
Figure~\ref{figs/earth-overview} is an overview.

The result of the forward pass is the MARS basis matrix \texttt{bx} and
the set of terms defined by \texttt{dirs}
and \texttt{cuts} (these are all fields in \texttt{earth}'s return value,
but the \texttt{bx} returned by the forward pass
includes all terms before trimming
back to \texttt{selected.terms}).

The \texttt{bx} matrix has a row for every observation (i.e. for every row in \texttt{x}).
It has a column for each basis function (also referred to as a MARS term).
% A basis function, or term,
% Each MARS term in printed by \texttt{summary.earth} is a
An example \texttt{bx}:
\small
\vspace{-2mm}
\begin{verbatim}
        (Intercept)   h(x1-58)   h(x2-89)   h(89-x2)   h(56-x3)*h(x1-58)  ...
   [1,]           1        3.2          0         56                   0  ...
   [2,]           1        8.1          0         55                   0  ...
   [3,]           1        3.7          0         54                   0  ...
   ....
\end{verbatim}
\vspace{- 3mm}
\normalsize

\begin{figure}[b]\centering
  \includegraphics[width=1\textwidth]{figs/earth-overview.jpg}
  \caption{\textit{Overview of} \texttt{earth}\textit{'s internals}
}
  \label{figs/earth-overview}
\end{figure}

\subsubsection{Termination conditions for the forward pass}
\label{sec-termination}

The forward pass adds terms in pairs until the first of the
following conditions is met:
\vspace{-4mm}
\begin{enumerate}[(i)]
\item Reached the maximum number of terms \texttt{nk}\vspace{-3mm}
\item Adding a term changes RSq by less than 0.001\vspace{-3mm}
\item Reached a RSq of 0.999 or more\vspace{-3mm}
\item GRSq is less than -10 (a pathologically bad GRSq, FAQs~\ref{faq05a} and~\ref{faq05b})\vspace{-3mm}
\item Reached numerical accuracy limits (no new term increases RSq).
\end{enumerate}
\vspace{-2mm}

Set \texttt{trace $>=$ 1} to see the stopping condition and
\texttt{trace $>=$ 2} to trace the forward pass.

The numbers 0.001 and 0.999 above can be changed by changing \texttt{earth}'s
\texttt{thresh} argument.
You can disable termination conditions ii, iii, and iv by setting
\texttt{thresh=0} (FAQ~\ref{faq10}).
These conditions are not theoretically necessary, but they save time by
terminating the forward pass when it is pointless to continue,
and also usually terminate the search before numerical issues become thorny.

Note that GCVs (via GRSq) are used during the forward pass only as one of the
(more unusual) stopping conditions and in \texttt{trace} prints.
Changing the \texttt{penalty} argument does not change the knot positions.

Setting \texttt{earth}'s argument \texttt{thresh} to zero is treated as a special case:
\texttt{thresh=0} disables all termination conditions except \texttt{nk}
and conditions involving numerical limits.
By disabling \texttt{thresh} we are allowing \texttt{earth} to
continue processing even if numerical issues can cause instability.
(This opens up the possibility of nonsensical RSq's and GRSq's
caused by numerical error.)

See also FAQ~\ref{faq10}.

\subsection{The pruning pass}
\label{sec-the-pruning-pass}

The pruning pass (also called the backward pass) is handed the set of terms
\texttt{bx} created by the forward pass.
Its job is to find the subset of those terms that gives the lowest GCV.
The following description of the pruning pass
explains how various fields in \texttt{earth}'s returned value are generated.

The pruning pass works like this:
it determines the subset of terms in \texttt{bx} (using \texttt{pmethod})
with the lowest RSS (residual sum-of-squares)
for each model size in \texttt{1:nprune}.
It saves the RSS and term numbers for each such subset in \texttt{rss.per.subset}
and \texttt{prune.terms}.
It then calculates the GCV with \texttt{penalty}
for each entry of \texttt{rss.per.subset} to yield \texttt{gcv.per.subset}.
Finally it chooses the model with the lowest value in
\texttt{gcv.per.subset}, puts its term numbers into \texttt{selected.terms},
and updates \texttt{bx} by keeping only the \texttt{selected.terms}.

After the pruning pass, \texttt{earth} runs \texttt{lm.fit} to
determine the \texttt{fitted.values}, \texttt{residuals}, and
\texttt{coefficients}, by regressing the response \texttt{y} on
\texttt{bx}.
This is an ordinary least-squares regression of the response \texttt{y}
on the basis matrix \texttt{bx} (see Figure~\ref{figs/earth-overview}
and \texttt{example (model.matrix.earth)} for an example).
If \texttt{y} has multiple columns then \texttt{lm.fit} is called
for each column.

If a \texttt{glm} argument is passed to \texttt{earth} (Chapter~\ref{sec-glm}),
\texttt{earth} runs \texttt{glm} on (each column of) \texttt{y}
in addition to the above call to \texttt{lm.fit}.


Set \texttt{trace $>=$ 3} to trace the pruning pass.

% By default \texttt{Get.crit} is \texttt{earth:::get.gcv}.
% Alternative \texttt{Get.crit} functions can be defined.
% See the source code of \texttt{get.gcv} for an example.

\subsection{Execution time}

For a given set of input data,
the following can increase the speed of the forward pass:
\vspace{-4mm}
\begin{enumerate}[(i)]
\item decreasing \texttt{degree} (because there are fewer
combinations of terms to consider),\vspace{-2mm}
\item decreasing \texttt{nk} (because there are fewer forward pass terms),\vspace{-2mm}
\item decreasing \texttt{fast.k} (because there are fewer potential parents to consider
at each forward step),\vspace{-2mm}
\item increasing \texttt{thresh} (faster if there are fewer forward pass terms),\vspace{-2mm}
\item increasing \texttt{min.span} (because fewer knots need to be considered).
\end{enumerate}
The backward pass is normally much faster than the forward pass,
unless \texttt{pmethod = "exhaustive"}.
Reducing \texttt{nprune} reduces exhaustive search time.
One strategy is to first build a large model
and then adjust pruning parameters such as \texttt{nprune} using
\texttt{update.earth}.

The following very rough rules of thumb apply for large models.
Using \texttt{minspan=1} instead of the default \texttt{0} will increase times by 20 to 50\%.
Using \texttt{fast.k=5} instead of the default \texttt{20} can give substantial speed gains
but will sometimes give a much smaller GRSq.

\subsection{Memory use}

Earth does not impose specific limits on the model size.
Model size is limited only by the amount of memory on your system,
the maximum memory addressable by R, and your patience.

To reduce total memory usage, it sometimes helps to remove variables
(use R's \texttt{remove} function) and to call \texttt{gc}
before invoking \texttt{earth}.  Note that increasing the \texttt{degree}
does not change the memory requirements (but increases the
running time).

Memory use will be minimized if ALL the following requirements are met
(because \texttt{earth} does not have to make its own copy of the
\texttt{x} matrix):
\vspace{-3mm}
\begin{enumerate}[(i)]
\item use \texttt{earth.default}, not \texttt{earth.formula}
(i.e. invoke \texttt{earth} with \texttt{x,y} not with a formula)\vspace{-2mm},

\item \texttt{x} must be a \texttt{matrix} of
\texttt{double}'s (not a \texttt{data.frame})\vspace{-2mm},

\item \texttt{x} must have column names\vspace{-2mm},

\item arguments like \texttt{subset} must be at their default value of
\texttt{NULL}\vspace{-2mm},

\item \texttt{trace} must be less than 2.
% (because \texttt{DUP=TRUE} is required to pass predictor names to
% \texttt{earth}'s internal call to \texttt{.C}).\vspace{-2mm}

\end{enumerate}
\vspace{-3mm}
The special value \texttt{trace=1.5} will make \texttt{earth}'s C
routines print memory allocations.

Memory requirements will be reduced if \texttt{Use.beta.cache=FALSE}
(use \texttt{trace=1.5} to see by how much).

\subsection{Standard model functions}

Standard model functions such as \texttt{case.names}
are provided for \texttt{earth} objects and are not explicitly documented.
Many of these give warnings when the results are not what you may expect.
Pass \texttt{warn=FALSE} to these functions to turn of just these warnings.
The full list of \texttt{earth} methods is:

\hspace*{10mm}\texttt{anova.earth},\newline
\hspace*{10mm}\texttt{case.names.earth},\newline
\hspace*{10mm}\texttt{deviance.earth},\newline
\hspace*{10mm}\texttt{effects.earth},\newline
\hspace*{10mm}\texttt{extractAIC.earth},\newline
\hspace*{10mm}\texttt{family.earth},\newline
\hspace*{10mm}\texttt{model.matrix.earth},\newline
\hspace*{10mm}\texttt{plot.earth},\newline
\hspace*{10mm}\texttt{print.earth},\newline
\hspace*{10mm}\texttt{print.summary.earth},\newline
\hspace*{10mm}\texttt{resid.earth},\newline
\hspace*{10mm}\texttt{residuals.earth},\newline
\hspace*{10mm}\texttt{summary.earth},\newline
\hspace*{10mm}\texttt{update.earth},\newline
\hspace*{10mm}\texttt{variable.names.earth}.

\subsection{Multiple response models}
\label{sec-multiple-responses}

If the response \texttt{y} has \texttt{k} columns
then \texttt{earth} builds \texttt{k} simultaneous models.
(Note: this will be the case if a factor in \texttt{y} is expanded by
\texttt{earth}; Chapter~\ref{sec-factors} ``Factors''.)
Each model has the same set of basis functions
(the same \texttt{bx}, \texttt{selected.terms}, \texttt{dirs} and \texttt{cuts})
but different coefficients (the returned \texttt{coefficients} will have \texttt{k} columns).
The models are built and pruned as usual but with the GCVs
and RSSs summed across all \texttt{k} responses.
Earth minimizes the overall GCV (the sum of the GCVs).

Once you have built your model, you can use \texttt{plotmo} and its
\texttt{nresponse} argument to see how each response
varies with the predictors.

Here are a couple of examples to show some of the ways
multiple responses can be specified.
Note that \texttt{data.frame}s can't be used on the left side
of a formula, so \texttt{cbind} is used in the first example.
The first example uses the standard technique of specifying
a tag \texttt{ly2=} to name a column.
\vspace{-4mm}
\begin{verbatim}
    earth(cbind(y1, ly2=log(y2)) ~ ., data = my.data)
    attach(my.data)
    earth(data.frame(y1, y2), data.frame(x1, x2, log.x3=log(x3))))
\end{verbatim}
\vspace{-3mm}
Since \texttt{earth} attempts to optimize for all models simultaneously,
the results will not be as ``good'' as building the models independently,
i.e., the GRSq of the combined model will usually not be as good as
the GRSq's for independently built models.
However, the combined model may be a better model in other senses,
depending on what you are trying to achieve.
For example, it could be useful for \texttt{earth} to select
the set of MARS terms that is best across \emph{all} responses.
This would typically be the case in a multiple response logistic model
if some responses have a very small number of successes.

Note that automatic scaling of \texttt{y} (via the \texttt{scale.y} argument)
does not take place if \texttt{y} has multiple columns.
You may want to scale your \texttt{y} columns before calling \texttt{earth}
so each \texttt{y} column gets the appropriate weight during model building
(a \texttt{y} column with a big variance will influence the
model more than a column with a small variance).
You could do this by calling \texttt{scale} before invoking \texttt{earth},
or by setting the \texttt{scale.y} argument, or by using
the \texttt{wp} argument.

Don't use a plus sign on the left side of the tilde.
You might think that specifies a multiple response, but instead
it arithmetically adds the columns.

For more details on using residual errors averaged over multiple responses
see for example
Section 4.1 of the FDA paper (Hastie, Tibshirani, and Buja~\cite{Hastie94}).

\subsection{Migrating from mda::mars}

Changing code from \texttt{mda::mars} to \texttt{earth}
is usually just a matter
of changing the call from \texttt{mars} to \texttt{earth}.
But there are a few argument differences and
\texttt{earth} will issue a warning if you give it a \texttt{mars}-only argument.

The resulting model will be similar but not identical because
of small implementation differences.
See also the documentation of
the function \texttt{mars.to.earth} (in the \texttt{earth} package).

If you are further processing the output of \texttt{earth} you will need to
consider differences in the returned value.  The header of the
source file \texttt{mars.to.earth.R} describes these.
Perhaps the most important is that \texttt{mars} returns the
MARS basis matrix in a field called "\texttt{x}"
whereas \texttt{earth} returns "\texttt{bx}".
Also, \texttt{earth} returns "\texttt{dirs}" rather than "\texttt{factors}".
% and in \texttt{earth} this matrix can have entries of value 2 for linear predictors
% (created with \texttt{earth}'s \texttt{linpreds} argument).

A note on \texttt{wp} argument.
Earth's internal normalization of \texttt{wp} is different from
\texttt{mars}.  Earth uses \texttt{wp <- sqrt(wp/mean(wp))} and \texttt{mars}
uses \texttt{wp <- sqrt(wp/sum(wp))}.  Thus in \texttt{earth}, a \texttt{wp}
with all elements equal is equivalent to no \texttt{wp}.  For models
built with \texttt{wp}, multiply the GCV calculated by \texttt{mars} by
\texttt{length(wp)} to compare it to \texttt{earth}'s GCV.

\clearpage
\section{Generalized linear models}
\label{sec-glm}

Earth builds a GLM model if the \texttt{glm} argument is specified.
Earth builds the model as usual and then invokes
\texttt{glm} on the MARS basis matrix \texttt{bx}.

In more detail, the model is built as follows.
Earth first builds a standard MARS model, including
the internal call to \texttt{lm.fit} on \texttt{bx} after the pruning pass.
(See Figure~\ref{figs/earth-overview} and
Section~\ref{sec-the-pruning-pass} ``The pruning pass''.)
Thus knot positions and terms are determined as usual and
all the standard fields in \texttt{earth}'s return value will be present.
Earth then invokes \texttt{glm} for the response on \texttt{bx}
with the parameters specified in the \texttt{glm} argument to \texttt{earth}.
For multiple response models
(when \texttt{y} has multiple columns), the call to \texttt{glm} is repeated independently for each response.
The results go into three extra fields in \texttt{earth}'s return value:
\texttt{glm.list}, \texttt{glm.coefficients}, and \texttt{glm.bpairs}.

Earth's internal call to \texttt{glm} is made with
the \texttt{glm} arguments \texttt{x}, \texttt{y}, and \texttt{model} set TRUE
(see the documentation for \texttt{glm} for more information about those arguments).
% [TODO Would like to make this depend on \texttt{keepxy} but currently \emph{have} to
% keep \texttt{x} and \texttt{y} else later calls to \texttt{plot.glm} on the GLM models fail.]

Use \texttt{summary(my.model)} as usual to see the model.
Use \texttt{summary(my.model, details=T)} to see more details, but
note that the printed t-values for the GLM coefficients are meaningless.
This is because of the amount of preprocessing by \texttt{earth} ---
the mantra is ``variable selection overstates significance of the selected variables''.
And anyway, we already know that the MARS terms are significant
--- the forward and backward passes just did a lot of work carefully
choosing those terms.

Use \texttt{plot(my.model\$glm.list[[1]])} to plot the (first) \texttt{glm} model.

The approach used for GLMs in \texttt{earth} was motivated by work done by
Jane Elith and John Leathwick
(\hspace{-.2mm}\cite{Leathwick} is a representative paper).

\subsection{GLM examples}
\label{glmexamples}

The examples below show how to specify earth-glm models.
The examples are only to illustrate the syntax and not necessarily useful models.
In some of the examples, \texttt{pmethod="none"}, otherwise with these artificial
models \texttt{earth} tends to prune away everything except the intercept term.
You wouldn't normally use \texttt{pmethod="none"}.
Also, \texttt{trace=1}, so if you run these examples you can
see how \texttt{earth} expands the input matrices
(as explained in Chapter~\ref{sec-factors} ``Factors''
and Section~\ref{sec-binomial-pairs} ``Binomial pairs'').

(i) \textbf{Two-level factor or logical response.}
The response is converted to a single column of 1s and 0s.
\vspace{-4mm}
\begin{verbatim}
    a1 <- earth(survived ~ ., data=etitanic,
                 degree=2, trace=1, glm=list(family=binomial))

    # equivalent but using earth.default
    a1a <- earth(etitanic[,-2], etitanic[,2],
                 degree=2, trace=1, glm=list(family=binomial))
\end{verbatim}

(ii) \textbf{Factor response.}
This example is for a factor with more than two levels.
(For factors with just two levels, see the previous example.)
The factor \texttt{pclass} is expanded to three indicator columns
(whereas in a direct call to \texttt{glm}, \texttt{pclass} would be treated
as logical: the first level versus all other levels).
Because of the ``masking problem'', we mention that you might consider FDA for
factor responses with more than two levels
(Chapter~\ref{sec-using-earth-with-fda-and-mda}).
\vspace{-4mm}
\begin{verbatim}
    a2 <- earth(pclass ~ ., data=etitanic, trace=1,
                glm=list(family=binomial))
\end{verbatim}

(iii) \textbf{Binomial model specified with a column pair.}
This is a single response model but specified with a pair of columns
(Section~\ref{sec-binomial-pairs} ``Binomial pairs'').
For variety, this example uses a \texttt{probit} link and (unnecessarily) increases \texttt{maxit}.
\vspace{-4mm}
\begin{verbatim}
    ldose <- rep(0:5, 2) - 2 # V&R 4th ed. p. 191
    sex <- factor(rep(c("male", "female"), times=c(6,6)))
    numdead <- c(1,4,9,13,18,20,0,2,6,10,12,16)
    pair <- cbind(numdead, numalive=20 - numdead)
    a3 <- earth(pair ~ sex + ldose, trace=1, pmethod="none",
                glm=list(family=binomial(link=probit), maxit=100))
\end{verbatim}

(iv) \textbf{Double binomial response} (i.e., a multiple response model)
specified with two column pairs.
\vspace{-4mm}
\begin{verbatim}
    numdead2 <- c(2,8,11,12,20,23,0,4,6,16,12,14) # bogus data
    doublepair <- cbind(numdead, numalive=20-numdead,
                        numdead2=numdead2, numalive2=30-numdead2)
    a4 <- earth(doublepair ~ sex + ldose, trace=1, pmethod="none",
                glm=list(family="binomial"))
\end{verbatim}

(v) \textbf{Poisson model.}
\vspace{-4mm}
\begin{verbatim}
    counts <- c(18,17,15,20,10,20,25,13,12) # Dobson 1990 p. 93
    outcome <- gl(3,1,9)
    treatment <- gl(3,3)
    a5 <- earth(counts ~ outcome + treatment, trace=1, pmethod="none",
                glm=list(family=poisson))
\end{verbatim}

(vi) \textbf{Standard earth model}, the long way.
\vspace{-4mm}
\begin{verbatim}
    a6 <- earth(numdead ~ sex + ldose, trace=1, pmethod="none",
                glm=list(family=gaussian(link=identity)))
    print(a6$coefficients == a6$glm.coefficients)  # all TRUE
\end{verbatim}

% [TODO investigate offsets in earth-glm models.]

\subsection{Binomial pairs}
\label{sec-binomial-pairs}

This section is only relevant if you use \texttt{earth}'s \texttt{glm} argument
with a binomial or quasibinomial \texttt{family}.

Users of the \texttt{glm} function will be familiar with
the technique of specifying a binomial response as a two-column matrix,
with a column for the number of successes and a column for the failures.
When given the argument \texttt{glm=list(family=binomial)},
\texttt{earth} automatically detects when such columns are present in \texttt{y}
(by looking for adjacent columns which both have entries greater than 1).
The first column only is used to build the standard \texttt{earth} model.
Both columns are then passed to \texttt{earth}'s internal call to \texttt{glm}.
As always, use \texttt{trace=1} to see how the columns
of \texttt{x} and \texttt{y} are expanded.

You can override this automatic detection by including a \texttt{bpairs} parameter.
This is usually (always?) unnecessary. For example
\vspace{-4mm}
\begin{verbatim}
    glm=list(family=binomial, bpairs=c(TRUE, FALSE))
\end{verbatim}
\vspace{-3mm}
specifies that there are two columns in the response with the second
paired with the first.
These examples
\vspace{-4mm}
\begin{verbatim}
    glm=list(family=binomial, bpairs=c(TRUE, FALSE, TRUE, FALSE))
    glm=list(family=binomial, bpairs=c(1,3)) # equivalent
\end{verbatim}
\vspace{-3mm}
specify that the 1st and 2nd columns are a binomial pair
and the 3rd and 4th columns another binomial pair.

\clearpage
\section{Factors}
\label{sec-factors}

This chapter explains how factors in the \texttt{x} and \texttt{y}
matrices get ``expanded'' before the matrices get passed to the MARS
engine.

Use \texttt{trace=1} or higher to see the column names of the
\texttt{x} and \texttt{y} matrices after factor expansion.
Use \texttt{trace=4} to see the first few rows of \texttt{x} and
\texttt{y} after factor expansion.

\subsection{Factors in x}
Earth treats factors in \texttt{x} in
the same way as standard R models such as \texttt{lm}.
Thus factors are expanded using the current setting
of \texttt{contrasts}.

\subsection{Factors in y}
Earth treats factors in the response in a non-standard way that makes use
of \texttt{earth}'s ability to handle multiple responses.

A \emph{two level factor} (or logical) is converted to a single indicator column of 1s and 0s.

A \emph{factor with three or more levels}
is converted into \texttt{k} indicator columns of 1s and 0s, where \texttt{k} is the number of levels
(the \texttt{contrasts} matrix is an identity matrix, see
the help page for \texttt{contr.earth.response}).
This happens regardless of the \texttt{options("contrasts")}
setting and regardless of whether the factors are ordered or unordered.
For example, if a column in \texttt{y} is a factor with levels
\texttt{A}, \texttt{B}, and \texttt{C},
the column will be expanded to three columns like this
(the actual data will vary but each row will have a single 1):
\vspace{-4mm}
\begin{verbatim}
    A B C  # one column for each factor level
    0 1 0  # each row has a single 1
    1 0 0
    0 0 1
    0 0 1
    0 1 0
    ...
\end{verbatim}
\vspace{-3mm}
In distinction, a standard treatment contrast
on the right hand side of say an \texttt{lm} model
with an intercept would have no first ``A'' column.
This is to prevent linear dependencies on the right side of the \texttt{lm} formula.
See the help page for \texttt{contrasts} for details.

This expansion to multiple columns (which only happens for factors with more than two levels)
means that \texttt{earth} will build a multiple response model
as described in Section~\ref{sec-multiple-responses} ``Multiple responses''.

% [TODO ordered factors should be treated differently?]
% [TODO should be able to pass a y contrasts argument to earth --- overkill?]

Paired binomial response columns in \texttt{y} are treated specially
(Section~\ref{sec-binomial-pairs} ``Binomial pairs'').

\subsection{Factor example}

Here is an example which uses the \texttt{etitanic} data to
predict the passenger class (not necessarily a sensible thing to do):
\vspace{-4mm}
\begin{verbatim}
    > data(etitanic)
    > head(etitanic) # pclass and sex are unordered factors

      pclass survived    sex    age sibsp parch
    1    1st        1 female 29.000     0     0
    2    1st        1   male  0.917     1     2
    3    1st        0 female  2.000     1     2

    > earth(pclass ~ ., data=etitanic, trace=1) # note col names in x and y below

    x is a 1046 by 5 matrix: 1=survived, 2=sexmale, 3=age, 4=sibsp, 5=parch
    y is a 1046 by 3 matrix: 1=1st, 2=2nd, 3=3rd
    rest not shown here...
\end{verbatim}
\vspace{-3mm}
% Note that earth does not automatically group categorical variables into subsets as
% described in Friedman 1991 \emph{Estimating Functions Of Mixed Ordinal and Categorical
% Variables Using Adaptive Splines}.

% \section{Balancing prediction error in multiple response models}
%
% If the number of occurrences of one class is much larger than the others,
% then earth, trying to minimize the overall error, will keep the
% error rate low on the large class while letting the error on
% the small classes increase.
% You can see when this happens by looking at the relative error rate for
% each class.
%
% To deal with this issue, use the \texttt{wp} argument to
% weight important but seldom occurring classes.
% TODO next sentence is misleading if y is a two level factor
% (Remember that after earth expands a factor with more than two levels,
% \texttt{y} will have a column for each class, and these
% are the columns weighted by \texttt{wp}.)
% Start off by making the weights inversely proportional to
% the class populations, build the model,
% look at the error rates, adjust the weights, and iterate.
%
% For more information (in a Random Forests setting) see
% Breiman and Cutler \emph{Random Forests}
% \url{www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#balance}.

\clearpage
\begin{figure}[b]
  \begin{center}
  \includegraphics[width=.7\textwidth, clip, trim=6mm 6mm 0mm 4mm, keepaspectratio]{figs/trees.pdf} % trim l b r t
  \caption{\textit{The} \texttt{linpreds} \textit{argument.
\newline
Top row: Standard} \texttt{earth} \textit{model of the trees data.
\newline
Bottom row: Same, but with Height entering linearly (}\texttt{linpreds=2}\textit{).
}}
  \label{figs/trees}
  \end{center}
\end{figure}

\section{The \texttt{linpreds} argument}

With \texttt{linpreds} we can specify which predictors should enter linearly,
instead of in hinge functions.
The \texttt{linpreds} argument does not stipulate that a predictor
\emph{must} enter the model, only that if it enters it should enter
linearly.
Starting with
\vspace{-4mm}
\begin{verbatim}
    fit1 <- earth(Volume ~ ., data = trees)
    plotmo(fit1)
\end{verbatim}
\vspace{-3mm}
we see in the \texttt{{plotmo}} graphs (Figure~\ref{figs/trees}, top row)
or by running \texttt{evimp}
that \texttt{Height} isn't as important as \texttt{Girth}.
For collaborative evidence that \texttt{Girth} is a more reliable
indicator of \texttt{Volume} we can use \texttt{pairs}:
\vspace{-4mm}
\begin{verbatim}
    pairs(trees, panel = panel.smooth)
\end{verbatim}
\vspace{-3mm}
Since we want the simplest model that describes the data, we may
decide that \texttt{Height} should enter linearly, not in a hinge
function (Figure~\ref{figs/trees}, bottom row):
\vspace{-4mm}
\begin{verbatim}
    fit2 <- earth(Volume ~ ., data = trees, linpreds = 2)  # 2 is Height column
    summary(fit2)
\end{verbatim}
\vspace{-3mm}
which yields
\vspace{-4mm}
\begin{verbatim}
                coefficients
    (Intercept)        2.981
    Height             0.348
    h(Girth-14)        6.302
    h(14-Girth)       -3.128
\end{verbatim}
\vspace{-3mm}
In this example, the second simpler model has almost the same RSS as the first model.
We can make both \texttt{Girth} and \texttt{Height} enter linearly with
\vspace{-4mm}
\begin{verbatim}
    a3 <- earth(Volume ~ ., data = trees, linpreds = c(1,2))
\end{verbatim}
\vspace{-3mm}
or with (the single TRUE is recycled to the length of \texttt{linpreds})
\vspace{-4mm}
\begin{verbatim}
    a4 <- earth(Volume ~ ., data = trees, linpreds = TRUE)
\end{verbatim}
\vspace{-3mm}
But specifying that all predictors should enter linearly is not really a useful thing to do.
In our simple example, the all-linear MARS model is the
same as a standard linear model
\vspace{-4mm}
\begin{verbatim}
    a5 <- lm(Volume ~ ., data = trees)
\end{verbatim}
\vspace{-3mm}
(compare the \texttt{summary} for each) but in general that will not be true.
Earth will not include a linear predictor if that predictor does not improve the model
(in the GCV sense).

Note: In the current implementation, the GCV penalty for predictors
that enter linearly is the same as that for predictors with knots.
That is not quite correct; linear terms should be penalized less.

% TODO Earth should have an argument to force variables into the model.

\clearpage
\section{The \texttt{allowed} argument}

You can specify how variables are allowed to enter MARS terms with the
\texttt{allowed} argument.
Within each step of the forward pass, \texttt{earth} calls the allowed
function after discovering the best knot for a variable.
The potential term is considered for inclusion only if the
\texttt{allowed} function returns \texttt{TRUE}.
The default function always returns \texttt{TRUE}.

Your \texttt{allowed} function should have the following prototype
\vspace{-4mm}
\begin{verbatim}
     function(degree, pred, parents, namesx, first)
\end{verbatim}
\vspace{-3mm}
where
\vspace{-4mm}
\begin{description}
\item[\texttt{degree}] is the interaction degree of the candidate term.
      Will be 1 for additive terms.\vspace{-2mm}

\item[\texttt{pred}] is the index of the candidate predictor.
     A predictor's index in \texttt{pred} is the column number in the input matrix \texttt{x}
     after factors have been expanded.
     Use \texttt{trace=1} to see the column names after expansion.\vspace{-2mm}

\item[\texttt{parents}] is the candidate parent term's row in \texttt{dirs}.\vspace{-2mm}

\item[\texttt{namesx}] is optional and if present is the column names of \texttt{x} after factors have been expanded.\vspace{-2mm}

\item[\texttt{first}] is optional and if present is TRUE the first time your \texttt{allowed} function is invoked for
the current model, and thereafter FALSE, i.e. it is TRUE once per invocation of \texttt{earth}.\vspace{-2mm}

\end{description}

\subsection{Examples}

The interface is flexible but requires a bit of programming.
We start with a simple example, which completely excludes one predictor
from the model:
\vspace{-4mm}
\begin{verbatim}
    example1  <- function(degree, pred, parents)   # returns TRUE if allowed
    {
        pred != 2  # disallow predictor 2, which is "Height"
    }
    a1 <- earth(Volume ~ ., data = trees, allowed = example1)
    print(summary(a1))
\end{verbatim}
\vspace{-3mm}
But that's not much use, because it's simpler to exclude the predictor
from the input matrix when invoking \texttt{earth}:
\vspace{-4mm}
\begin{verbatim}
    a2 <- earth(Volume ~ . - Height, data = trees)
\end{verbatim}
\vspace{-3mm}
The example below is more useful.
It prevents the specified predictor
from being used in interaction terms.
(The example is artificial because it's unlikely we
would want to single out humidity from interactions in the ozone data.)

The \texttt{parents} argument is the candidate parent's row in the \texttt{dirs} matrix
(\texttt{dirs} is described in the \texttt{Value} section of the \texttt{earth} help page).
Each entry of \texttt{parents} is 0, 1, -1, or 2, and we index
\texttt{parents} on the predictor index.
Thus \texttt{parents[pred]} is non-zero if \texttt{pred} is in the parent term.
\vspace{-4mm}
\begin{verbatim}
    example2 <- function(degree, pred, parents)
    {
        # disallow humidity in terms of degree > 1
        # 3 is the "humidity" column in the input matrix
        if (degree > 1 && (pred == 3 || parents[3]))
            return(FALSE)
        TRUE
    }
    a3 <- earth(O3 ~ ., data = ozone1, degree = 2, allowed = example2)
    print(summary(a3))
\end{verbatim}
\vspace{-3mm}
The following example allows only the specified predictors
in interaction terms.
Interactions are allowed only for predictors in \texttt{allowed.set},
which you can change to suit your needs.
\vspace{-4mm}
\begin{verbatim}
    example3 <- function(degree, pred, parents)
    {
        # allow only humidity and temp in terms of degree > 1
        # 3 and 4 are the "humidity" and "temp" columns
        allowed.set = c(3,4)
        if (degree > 1 &&
               (all(allowed.set != pred) || any(parents[-allowed.set])))
            return(FALSE)
        TRUE
    }
    a4 <- earth(O3 ~ ., data = ozone1, degree = 2, allowed = example3)
    print(summary(a4))
\end{verbatim}

\subsection{Further notes on the \texttt{allowed} argument}
The basic MARS model building strategy is always
applied even when there is an \texttt{allowed} function.
For example, \texttt{earth} considers a term for addition only
if all factors of that term except the new one are already in a model term.
This means that an \texttt{allowed} function that inhibits, say, all degree 2
terms will also effectively inhibit higher degrees too, because
there will be no degree 2 terms for \texttt{earth} to extend to degree 3.

You can expect model building to be about 10\% slower with an \texttt{allowed} function
because of the time taken to invoke the \texttt{allowed} function.
On the other hand, execution time can be faster
if using \texttt{allowed} requires us to evaluate fewer potential MARS terms.

\subsection{Using predictor names instead of indices in the \texttt{allowed} function.}
You can use predictor names instead of indices using
the optional \texttt{namesx} argument.
If present, \texttt{namesx} is the column names of \texttt{x} after factors have been expanded.
The first example above (the one that disallows \texttt{Height}) can be rewritten as
\vspace{-4mm}
\begin{verbatim}
    example1a  <- function(degree, pred, parents, namesx)
    {
        namesx[pred] != "Height"
    }
\end{verbatim}
\vspace{-3mm}
Comparing strings is inefficient [no longer true in modern versions
of R] and the above example can be rewritten a little more efficiently
using the optional \texttt{first} argument.
If present, this is TRUE the first time your allowed function is called for
the current model and thereafter FALSE.
\vspace{-4mm}
\begin{verbatim}
    iheight <- 0    # column index of "Height"

    example1b  <- function(degree, pred, parents, namesx, first)
    {
        if (first) {
            # first time this function is invoked, so
            # stash column index of "Height" in iheight
            iheight <<- which(namesx == "Height")   # note use of <<- not <-
            if (length(iheight) != 1) # sanity check
                stop("no Height in ", paste(namesx, collapse=" "))
        }
        pred != iheight
    }
\end{verbatim}
\clearpage
\begin{figure}[b]
  \begin{center}
  \includegraphics[width=.95\textwidth, clip, trim=2mm 85mm 2mm 4mm, keepaspectratio]{figs/fda.pdf} % trim l b r t
  \caption{\textit{
Left: FDA of the iris data, built on a linear model.\newline
Right: FDA built with an }\texttt{earth}  \textit{model using the code in the text.
Note the better grouping of classes.
\newline
\newline
The graphs show the training observations
transformed into the discriminant space.
This transformation is done by the regression function plugged into
}\texttt{fda} \textit{and by optimal scoring (Section~\ref{sec-intro-fda}).
There are three classes in this example
so we have two discriminant variables.
A new observation is classified by }\texttt{predict.fda} \textit{as the class
of the nearest centroid in discriminant space (the centroids are at the
ringed numbers 1, 2, and 3).
  }}
  \label{figs/fda}
  \end{center}
\end{figure}
\section{Using \texttt{earth} with \texttt{fda} and \texttt{mda}}
\label{sec-using-earth-with-fda-and-mda}
Earth can be used with \texttt{fda} and \texttt{mda}
in the \texttt{mda} package.
Earth will generate a multiple response model
when called by these functions.
You can pass arguments such as \texttt{degree=2}
to \texttt{earth} by including them in the call to \texttt{fda}.
Use the \texttt{earth} argument \texttt{keepxy=TRUE} if you want to call
\texttt{plotmo} later.
Use the \texttt{fda/mda} argument \texttt{keep.fitted=TRUE} if
you want to call \texttt{plot.earth} later
(actually only necessary for large data sets, see the description of
\texttt{keep.fitted} in \texttt{fda}'s help page).

Example (this gives the right side of Figure~\ref{figs/fda}):
\vspace{-4mm}
\begin{verbatim}
library(mda)
(fda <- fda(Species~., data=iris, keep.fitted=TRUE, method=earth, keepxy=TRUE))
summary(fda$fit)  # examine earth model embedded in fda model
plot(fda)         # right side of Figure 3
\end{verbatim}
\clearpage
Using \texttt{plotmo} we can plot the per-predictor dependence of
the \texttt{fda} variates like this:
\vspace{-4mm}
\small
\begin{verbatim}
    plotmo(fda, type="variates", nresponse=1, clip=F) # 1st disc var (Figure 5)
    plotmo(fda, type="variates", nresponse=2, clip=F) # 2nd disc var (not shown)
\end{verbatim}
\vspace{-3mm}
\normalsize
We can also look at the \texttt{earth} model embedded in the FDA model:
\vspace{-4mm}
\small
\begin{verbatim}
    plotmo(fda$fit, nresponse=1, clip=F) # earth in FDA, 1st disc var (Figure 6)
    plotmo(fda$fit, nresponse=2, clip=F) # earth in FDA, 2nd disc var (not shown)
\end{verbatim}
\normalsize
\vspace{-3mm}
% This model is additive (no predictor interactions) so the
% \texttt{plotmo} plots are quite informative even though we are seeing
% only a thin slice of the data.

\begin{figure}[b]
  \begin{center}
  \includegraphics[width=.88\textwidth, clip, trim=2mm 135mm 0mm 3mm, keepaspectratio]{figs/fda-lm.pdf} % trim l b r t
  \caption{\texttt{plotmo} \textit{graphs of the FDA model with a linear submodel
(the left of Figure~3).\newline
The graphs show the contribution of each predictor
to the first discriminant variable.\newline
The second discriminant variable is not shown. (The code to generate this plot is not in the text.)
}}
  \label{figs/fda-lm}
  \end{center}
\end{figure}

\begin{figure}[b]
  \begin{center}
  \includegraphics[width=.88\textwidth, clip, trim=2mm 135mm 0mm 3mm, keepaspectratio]{figs/fda-plotmo.pdf} % trim l b r t
  \caption{\texttt{plotmo} \textit{graphs of the FDA model with an }\texttt{earth} \textit{submodel
(the right of Figure~\ref{figs/fda}).
The graphs show the contribution of each predictor
to the first discriminant variable.}
% The main contributor by far is }\texttt{Petal.Length}\textit{.
% \newline
% \texttt{sepal.width} \textit{is not used in the model, but }
% \texttt{plotmo} doesn't know that, so plots it anyway.
}
  \label{figs/fda-plotmo}
  \end{center}
\end{figure}

\begin{figure}[b]
  \begin{center}
  \includegraphics[width=.88\textwidth, clip, trim=2mm 135mm 0mm 3mm, keepaspectratio]{figs/fda-earth.pdf} % trim l b r t
  \caption{\texttt{plotmo} \textit{graphs of
the }\texttt{earth}\textit{ model embedded in the FDA model
(first discriminant variable before scoring).
Earth did not include }\texttt{Sepal.Width}\textit{ in the model.
}}
  \label{figs/fda-earth}
  \end{center}
\end{figure}

\subsection{A short introduction to Flexible Discriminant Analysis}
\label{sec-intro-fda}

Flexible Discriminant Analysis (FDA) is Linear Discriminant Analysis
(LDA) on steroids.
LDA uses a hyperplane to separate the classes.
FDA replaces this hyperplane with a curved or bent surface to better
separate the classes.
The trick FDA uses to achieve this is to convert the classification
problem into a regression problem.
This allows us to plug in ``any'' regression function to generate the
discriminant surface.
If we plug in a linear regression function, FDA will generate
a hyperplane, just like LDA.
% , and not much is gained.\footnote[1]{That's
% not true.  Even though the discriminant surface is indeed merely a
% plane, FDA's optimal scoring might still allow a better separation of
% classes than LDA.}
If we plug in \texttt{earth}, FDA will generate a surface
defined by MARS hinge functions.

FDA converts a classification problem into a regression problem
via \emph{optimal scoring} (Figure~\ref{figs/scoring}).
Essentially, this creates a new response
variable by assigning new numbers (scores) to the factor levels in the
original response.
So for example \texttt{setosa=1}, \texttt{versicolor=2}, and
\texttt{virginica=3} may become
\texttt{setosa=1.2}, \texttt{versicolor=-1.2}, and \texttt{virginica=0}.
% FDA rescores the response both before and after calling the regression
% function.

Actually, FDA creates several response variables like this, each
with its own set of scores.
If there are $K$ response classes, FDA creates $K-1$ variables.
So for the Iris data set, which has three classes (or ``levels'' in R
parlance), we have two discriminant variables (Figure~\ref{figs/fda}).
For a binary response FDA creates one discriminant variable,
and the discriminant space is one dimensional.
Note that the dimension of the discriminant space depends on the
number of classes, not on the number of predictors --- a nice example
of dimensionality reduction.
Sometimes the best prediction results on independent data are obtained if we use
only some of the discriminant variables, and thus a further reduction
in dimensionality is possible.

% (Thus the regression function must be able to deal with multiple responses.
% An example is \texttt{earth}; use \texttt{trace=4} to see the
% optimally scored responses handed to \texttt{earth} by \texttt{fda}.)

Further details may be found in Hastie et al.~\cite{ESL} Section 12.5
and the FDA paper~(Hastie, Tibshirani, and Buja~\cite{Hastie94}).

\begin{figure}[b]
  \begin{center}
  \includegraphics[width=1\textwidth, clip, trim=2mm 138mm 80mm 4mm, keepaspectratio]{figs/scoring.pdf} % trim l b r t
  \caption{\textit{Left: Some toy data with three response categories.\newline
Right: Rescoring assigns a new number to each category
so the data are in a better form for linear separation.
  }}
  \label{figs/scoring}
  \end{center}
\end{figure}

Using FDA is usually recommended for a response that is a
factor with more than two levels, rather than using
a regression function like \texttt{lm} or \texttt{earth}
directly on an indicator matrix.
This is because of the ``masking problem''
(e.g.\ Hastie et al.~\cite{ESL} Section 4.2 ``Linear Regression of an
Indicator Matrix'').
In practice the best advice is try it and see if you get better results
on your data.

Two advantages of FDA are (i) FDA will often perform better than LDA (or QDA)
because it generates a flexible surface to separate the classes, and
(ii) for responses which have more than two levels,
FDA will often perform better than regression on an indicator
matrix because it does not suffer from the masking problem.

We mention that the acronym FDA for ``Flexible Discriminant Analysis''
is not to be confused with the same acronym for ``Functional Data
Analysis''~\cite{fda}.

TODO When is FDA exactly equivalent to LDA?

% \subsection{An implementation note}

% The \texttt{method} argument to \texttt{fda} must be a regression
% function that has an \texttt{x}, \texttt{y} interface.
% The regression function must allow multiple responses
% (i.e., \texttt{y} can have multiple columns).
% And the regression function must have a
% \texttt{predict} method.  Unfortunately the standard routines in R for
% linear regression, \texttt{lm} and \texttt{lm.fit}, do not meet these
% requirements (\texttt{lm} has a formula interface and \texttt{lm.fit}
% does not have a \texttt{predict} method).
% The \texttt{mda} package provides a routine \texttt{polyreg} that
%  can be used instead.

\clearpage
\section{The \texttt{plot.earth} function}

This chapter describes the graphs produced by \texttt{plot.earth}.

\subsection{Short version of this chapter}

For readers who do not wish to read this entire chapter, here is the
least you need to know.

The \texttt{plot.earth} function produces four graphs (Figure~\ref{figs/plot-earth}).

Use the Model Selection plot to see
how the fit depends on the number of predictors,
how the final model was selected at the maximum GCV,
and so on.

Use the Residuals vs Fitted graph to look for outliers and for any
obviously strange behavior of the fitted function.

You can usually ignore the other two graphs.

\subsection{Interpreting the \texttt{plot.earth} graphs}

The graphs plotted by \texttt{plot.earth}, apart from the Model Selection plot, are
standard tools used in residual analysis and more information
can be found in most linear regression textbooks.

Heteroscedasity of the residuals isn't as important with
\texttt{earth} models as it is with linear models, where homoscedasity
of the (studentized) residuals is used a check that a linear model is appropriate.
Also, in linear models homoscedasity of the residuals is required for
the usual linear model inferences (such as calculation of p-values), which is not
done with \texttt{earth} models.

Remember that the residuals are measured on the training data rather
than on new data.  In linear models that is usually not an issue, but
for flexible models like MARS the residuals measured on the training
data give an optimistic view of the model's predictive ability.

\subsubsection{Nomenclature}
% The \emph{response value} is the observed response value.
% The \emph{fitted value} is a synonym for the \emph{predicted value} on the training data.
The \emph{residuals} are the differences between the values predicted by the model
and the corresponding response values.
The \emph{residual sum of squares} (RSS) is the sum of the squared values of the residuals.

\emph{R-Squared} (\texttt{RSq}, also called the \emph{coefficient of determination})
is a normalized form of the RSS,
and, depending on the model, varies from 0
(a model that always predicts the same value i.e.\ the mean observed response value)
to 1 (a model that perfectly predicts the responses in the training
data).\footnote[1]{Not quite true, see FAQ~\ref{faq05} ``Can R-Squared be negative?''}

The \emph{Generalized Cross Validation} (GCV)
is a form of the RSS penalized by the effective number of model parameters
(and divided by the number of observations).
More details can be found in FAQs \ref{faq06} and \ref{faq07}.
The \emph{GRSq} normalizes the GCV in the same way that the RSq normalizes
the RSS
(see FAQ~\ref{faq05a}
and the definition of GRSq in the \texttt{Value} section of
\texttt{earth}'s help page).

The GCV and GRSq are measures of the generalization ability of the model,
i.e., how well the model would predict using data not in the training set.
There is some arbitrariness in their values since the effective
number of model parameters is a just an estimate in MARS models.

\begin{figure}
  \begin{center}
  \includegraphics[width=1.1\textwidth, clip, trim=4mm 4mm 2mm 0mm, keepaspectratio]{figs/plot-earth.pdf} % trim l b r t
  \caption{\textit{Graphs produced by }\texttt{example(plot.earth)}\textit{.}
}
  \label{figs/plot-earth}
  \end{center}
\end{figure}

\subsubsection{The Model Selection graph}

For concreteness, the description of the graphs here is based on the
plot produced by \texttt{example(plot.earth)}
(Figure~\ref{figs/plot-earth}).

In the example Model Selection graph
(top left of Figure~\ref{figs/plot-earth})
the RSq and GRSq lines run together at first, but diverge as the number of terms increases.
This is typical behavior, and what we are seeing is an increased penalty
being applied to the GCV as the number of model parameters increases.

The vertical dotted line is positioned at the selected model
(at the maximum GRSq unless \texttt{pmethod="none"} was used)
and indicates that the best model has 11 terms and uses all 8 predictors
(the number of predictors is shown by the black dashed line).

We can also see the number of predictors and terms we would need
if we were prepared to accept a lower GRSq (you can use the
\texttt{earth} parameter \texttt{nprune} to trim the model).

To reduce clutter and the right-hand axis, use \texttt{col.npreds=0}.

\subsubsection{The Residuals vs Fitted graph}

The Residuals vs Fitted graph
(bottom left of Figure~\ref{figs/plot-earth})
shows the residual for each value of the predicted response.
By comparing the scales of the axes one can get an immediate
idea of the size of the residuals relative to the predicted values.

The pale blue line is a \texttt{loess} fit.
(Readers not familiar with \texttt{loess} fits can think of them as fancy moving averages.)
In this instance it shows that the mean residual is more or less constant
except at low fitted values.
The end effect is possibly due to failure of the model in that region because
of smaller residuals.
The model fitting algorithm will not try hard to improve the fit
on the left because there is little reduction in RSS
to be gained in that area.

Ideally the residuals should show constant variance
i.e.\ the residuals should remain evenly spread out,
or homoscedastic, as the fitted values increase.
(However, in flexible models like \texttt{earth}, constant variance of
the residuals isn't as important as it is in linear models.)
In the example graph we see heteroscedasity --- the residuals spread out in a ``$<$'' shape.
There is a decrease in the accuracy of the predictions as the predicted value increases.
% This indicates that variable selection in the MARS pruning pass
% will be biased towards large fitted values, i.e.,
% rather than treating the full range of the response fairly, the pruning pass
% in our example model will favor MARS terms that better fit large response values.
% (Variable selection is done using linear fits on the MARS basis functions.
% Variable selection is based on comparing GCVs and GCVs are derived from RSSs.
% In our example model, the bigger residuals at larger fitted values
% imply that larger fitted values have more ability to affect the RSS.)

To reduce the heteroscedasity,
we can refit the model after performing a transform on the response.
A cube root transform, for instance, evens out the residuals
(Figure~\ref{figs/cube-transform}, middle plot):
\vspace{-4mm}
\begin{verbatim}
    fit <- earth(O3^.33333 ~ ., data = ozone1, degree = 2)
    plot(fit)
\end{verbatim}
\vspace{-3mm}
Transforming the data may cause other problems,
such as mismatches to a known underlying physical model or
difficulties in interpretation,
so it's best to consult (or become) an expert on the type of data being modeled
(in this case, ozone pollution data --- an expert may say that taking
the cube root is meaningless, or conversely may say that it is essential).

\begin{figure}
  \begin{center}
  \includegraphics[width=1\textwidth, clip, trim=2mm 120mm 2mm 0mm, keepaspectratio]{figs/cube-transform.pdf} % trim l b r t
  \caption{\textit{
Three models built from the ozone data.\newline
Left: The residuals of an \texttt{earth} model (same as bottom left
of Figure~\ref{figs/plot-earth}).
\newline
Middle: The residuals of an \texttt{earth} model built on the cube root of the response.
\newline
Right: The residuals of a linear model.
}}
  \label{figs/cube-transform}
  \end{center}
\end{figure}

Compare the residuals of the \texttt{earth} model to the linear model
(Figure~\ref{figs/cube-transform}, right plot), and notice how the
smooth line shows that the \texttt{earth} model is more successful at
modeling non-linearities in the data.
Also, the \texttt{earth} residuals are smaller --- look at the left hand axis labels.
The code for the linear model plot is:
\vspace{-4mm}
\begin{verbatim}
    fit.lm <- lm(O3 ~ ., data = ozone1)
    plot(fit.lm, which=1)
\end{verbatim}
\vspace{-3mm}
One should always look at the residuals themselves as well as looking at
the \texttt{loess} fit, which is itself an approximation.
However, in the example plot the \texttt{loess} line appears reliable.

Cases 192, 193, and 226 have the largest residuals and
fall suspiciously into a separate cluster.
(If overplotting makes the labels hard to read,
reduce the number of labels with the \texttt{id.n} argument of \texttt{plot.earth}.)
As a general rule, it is worthwhile investigating cases with large residuals.
Perhaps they should be excluded when building the model.
Conversely, it is possible that they reveal something important about the data
that could warrant changes to the model.
In our example it is also worthwhile looking at cases
with \emph{small} residuals because of non-linearity in that region.
To see the example input matrix ordered on the magnitude of the residuals,
use \texttt{ozone1[order(abs(fit\$residuals)),]}.

Sometimes groups of residuals appear in a series of parallel lines.
These lines usually do not indicate a problem.
They are formed when a set of plotted points has the same observed
value, commonly due to discretization in the measurement of the
observed response (e.g.\ by rounding to the nearest inch).

\subsubsection{The Cumulative Distribution graph}

The Cumulative Distribution graph
(top right of Figure~\ref{figs/plot-earth})
shows the cumulative distribution of the absolute values of residuals.
What we would ideally like to see is a graph that starts at 0 and shoots up quickly to 1.
In the example graph, the median absolute residual is about 2.2
(look at the vertical gray line for 50\%).
We see that 95\% of the absolute values of residuals are less than about 7.1
(look at the vertical gray line for 95\%).
So in the training data,
95\% of the time the predicted value is within 7.1 units of the observed value.

\subsubsection{The QQ graph}

The QQ (quantile-quantile) plot
(bottom right of Figure~\ref{figs/plot-earth})
compares the distribution of the residuals to a normal distribution.
If the residuals are distributed normally they will lie on the line.
(Normality of the residuals isn't too important for \texttt{earth} models,
but the graph is useful for discovering outlying residuals and other anomalies.)
Following R convention, the abscissa is the normal axis
and the ordinate is the residual axis;
some popular books have it the other way round.
In the example,
we see divergence from normality in the left tail ---
the left tail of the distribution is fatter than that of a normal distribution.
Once again, we see that cases 192, 193, and 226 have the largest residuals.

\subsection{Earth-glm models and \texttt{plot.earth}}

The \texttt{plot.earth} function ignores the \texttt{glm} part of the
model, if any.
(``Earth-glm'' models are models created with \texttt{earth}'s \texttt{glm} argument,
Chapter~\ref{sec-glm}.)
The plotted residuals are residuals from \texttt{earth}'s call to
\texttt{lm.fit} after the pruning pass, not \texttt{glm}
residuals.

\begin{SCfigure}[][b]
  \begin{centering}
  \includegraphics[width=.5\textwidth, clip, trim=2mm 90mm 88mm 0mm, keepaspectratio]{figs/plotd-example.pdf} % trim l b r t
  \caption{\textit{Example of the }\texttt{plotd} \textit{function.}
\newline
\newline
\newline
\newline
\newline
\newline
}
  \label{figs/plotd-example}
  \end{centering}
\end{SCfigure}
For earth-glm models, \texttt{plotd} (in the \texttt{earth} package) can be convenient.
Example (Figure~\ref{figs/plotd-example}):
\vspace{-4mm}
\begin{verbatim}
fit <- earth(survived ~ ., data=etitanic, degree=2, glm=list(family=binomial))
plotd(fit, hist=TRUE)
\end{verbatim}
\vspace{-3mm}
\clearpage
We can plot the \texttt{glm} model inside \texttt{earth} like this (not shown):
\vspace{-4mm}
\begin{verbatim}
data(etitanic)
fit <- earth(survived~., data=etitanic, glm=list(family=binomial))
par(mfrow=c(2,2))
plot(fit$glm.list[[1]])
\end{verbatim}

\subsection{Cross-validated models and \texttt{plot.earth}}
\label{sec-cv-and-plot}

Earth builds cross-validated models with the \texttt{nfold} argument
(Chapter~\ref{sec-cross-validating} ``Cross-Validation'').
The Model Selection plot will show cross-validation statistics,
but only if \texttt{keepxy=TRUE} was also used when building the model.
(The cross-validation statistics are ignored in the other plots
generated by \texttt{plot.earth}.)
Here is an example
(Figure~\ref{figs/cv-plot-earth}):
\vspace{-4mm}
\begin{verbatim}
fit <- earth(survived ~ ., data = etitanic, degree=2, nfold=5, keepxy=T)
plot(fit, which=1, col.rsq=0)    # which=1 for Model Selection plot only
\end{verbatim}
\vspace{-3mm}

In Figure~\ref{figs/cv-plot-earth}, as usual the vertical black dotted
line shows the optimum number of terms determined as usual by the peak
GCV.

The pale pink lines show the out-of-fold RSq's for each fold model.
The red line is the mean out-of-fold RSq for each model size.

The vertical red dotted line is at the maximum of the red line,
i.e., the vertical line shows the optimum number of terms determined
by cross-validation.
This is \emph{CV-with-averaging}; another approach (not supported by
\texttt{plot.earth}) is \emph{CV-with-voting} which uses the modal
number-of-terms, i.e., the number-of-terms that is most often selected
at a fold.

\begin{SCfigure}[][b]
  \begin{centering}
  \includegraphics[width=.6\textwidth, clip, trim=0mm 90mm 88mm 0mm, keepaspectratio]{figs/cv-plot-earth.pdf} % trim l b r t
  \caption{\textit{A cross-validated }\texttt{earth}\textit{ model
  }
\newline
\newline
\newline
\newline
\newline
\newline
\newline
\newline
}
  \label{figs/cv-plot-earth}
  \end{centering}
\end{SCfigure}
\begin{figure}
  \begin{center}
  \includegraphics[width=1\textwidth, clip, trim=0mm 120mm 2mm 0mm, keepaspectratio]{figs/cv-variation.pdf} % trim l b r t
  \caption{\textit{The same model, cross-validated three times.
    Note the random variation in the cross-validation \texttt{RSq}'s as} \texttt{earth}
    \textit{partitions the data into folds differently for each run.
  }}
  \label{figs/cv-variation}
  \end{center}
\end{figure}
Ideally the number of terms selected using GRSq would always
match the number of terms determined by cross-validation, and the two
vertical lines coincide
(although \texttt{plot.earth} slightly jitters the lines
very slightly to prevent overplotting).
In reality, the vertical lines are usually close but not identical.
In the following example, note how the graph varies as the
cross-validation folds vary in each invocation of \texttt{earth}
(Figure~\ref{figs/cv-variation}):
\vspace{-4mm}
\begin{verbatim}
    plot1 <- function()
    {
        fit <- earth(survived~., data = etitanic, degree=2,
                     nfold=5, keepxy=TRUE)
        plot(fit, which=1, ylim=c(0, .5), col.rsq=0)
    }
    plot1()
    plot1()
    plot1()
\end{verbatim}
\vspace{-3mm}
The above code keeps the vertical axis range
constant across all three graphs with \texttt{ylim=c(0, .5)},
and reduces clutter with \texttt{col.rsq=0}.

We mention that in Figures~\ref{figs/cv-plot-earth} and \ref{figs/cv-variation}
the cross-validation results are
consistent with the results obtained in the standard way using the GCV.
The solid black and red lines are very close.
The vertical dotted red line dances around, but mostly because of the
flatness of the curve after about 6 terms.
If we used the one-standard-error rule (not yet supported by
\texttt{earth}), the position of the vertical line would be much more
stable.

% \subsection{I want to add lines or points to the RSq plot,
% and am having trouble getting my axis scaling right. Help?}

% Use \texttt{do.par=FALSE}.
% With \texttt{do.par=FALSE}, the axis scales match the axis labels.
% With \texttt{do.par=TRUE}, \texttt{plot.earth} restores the
% \texttt{par} parameters and axis scales to
% what they were before calling \texttt{plot.earth}.
% This usually means that the x- and y-axis scales are both 0 to 1.

\clearpage
\section{Estimating variable importance}
\label{evimp}

This chapter discusses how to estimate the relative importance of
variables in an \texttt{earth} model.
It was written in response to email about \texttt{evimp}.

\subsection{Introduction to variable importance}
\label{sec-intro-evimp}

What exactly is variable importance?
A working definition is that a variable's importance is a measure
of the effect that observed changes to the variable have on the observed response
(or better, the expectation of that effect over the population).
It is this measure of importance that \texttt{evimp} tries to estimate.

You might say that we can measure a variable's importance
by changing the variable's value and measuring how the response changes.
Indeed, the fact that an \texttt{earth} model is represented by an
equation seems to imply that this is the way to go.
However, except in special situations, there are problems with this way
of thinking because:

(i) It assumes we can change the variable, which is usually not the case.
For example, in the \texttt{trees} data,
we cannot simply generate a new tree of arbitrary height.

(ii) It assumes that changes to a variable can occur in isolation.  In
practice, a variable is usually tied to other variables, and a change
to the variable would never occur in the population without
simultaneous changes to other variables.
For example, in the \texttt{trees} data, a change in height is
associated with a change in the girth.

(iii) It implies a causal relationship, which often is not the case.
Changing the amount of mud does not tell us anything about the amount of rain.

Thus is is better to think in terms of the effect of the variable on
the response averaged over the entire population.
That is to say, the \emph{expected} effect.
In practice, we have to figure out how to use the model and the sample
as a surrogate for the population, which isn't trivial.

Note that variable importance in the \emph{equation that MARS derives
from the data} is not really what we have in mind here.
For example, if two variables are highly correlated,
MARS will usually drop one when building the model.
Both variables have the same importance in the data but
not in the MARS equation (one variable does not even appear in the equation).
Section~\ref{sec-evimp-plotmo} has a few words on how to use \texttt{plotmo}
to estimate variable importance in the MARS equation.

\subsection{Estimating variable importance}

Estimating predictor importance is in general a tricky and even controversial problem.
There is usually no completely reliable way to estimate the importance of the variables
in a standard MARS model.
% unless we make further lengthy tests after the model is built
% (lengthy tests such as cross-validation
% or leave-one-out techniques ---
% see Section~\ref{sec-building-many}).
The \texttt{evimp} function just makes an educated (and in practice useful)
estimate as described below.

\subsection{Three criteria for estimating variable importance}

The \texttt{evimp} functions uses three criteria for estimating
variable importance in a MARS model.

(i) The \texttt{nsubsets} criterion counts the number of model subsets that include the variable.
Variables that are included in more subsets are considered more important.

By "subsets" we mean the subsets of terms generated by the pruning pass.
There is one subset for each model size (from 1 to the size of the selected model)
and the subset is the best set of terms for that model size.
(These subsets are specified in \texttt{\$prune.terms} in \texttt{earth}'s return value.)
Only subsets that are smaller than or equal in size to the final model are used
for estimating variable importance.

(ii) The \texttt{rss} criterion first calculates the decrease in the RSS
for each subset relative to the previous subset.
(For multiple response models, RSS's are calculated over all responses.)
Then for each variable it sums these decreases over all subsets that include the variable.
Finally, for ease of interpretation the summed decreases are scaled so the largest summed decrease is 100.
Variables which cause larger net decreases in the RSS are considered more important.

(iii) The \texttt{gcv} criterion is the same, but uses the GCV instead of the RSS.
Adding a variable can \emph{increase} the GCV,
i.e., adding the variable has a deleterious effect on the model.
When this happens, the variable could even have a negative total importance,
and thus appear less important than unused variables.

Note that using RSq's and GRSq's instead of RSS's and GCV's
would give identical estimates of variable importance,
because \texttt{evimp} calculates \emph{relative} importances.

\subsection{Example}

This code
\vspace{-4mm}
\begin{verbatim}
    fit <- earth(O3 ~ ., data=ozone1, degree=2)
    evimp(fit, trim=FALSE)  # trim=FALSE to show unused variables
\end{verbatim}
\vspace{-3mm}
prints the following:
\vspace{-6mm}
\begin{verbatim}
              nsubsets   gcv    rss
    temp            10 100.0  100.0
    humidity         8  35.6   38.4
    ibt              8  35.6   38.4
    doy              7  33.6   36.0
    dpg              5  25.9   28.0
    ibh              4  30.9>  32.3>
    vis              4  20.8   22.9
    wind             1   8.7    9.9
    vh-unused        0   0.0    0.0
\end{verbatim}
\vspace{-3mm}
The rows are sorted on \texttt{nsubsets}.
We see that \texttt{temp} is considered the most important variable,
followed by \texttt{humidity}, and so on.
We see that \texttt{vh} is unused in the final model,
and thus is given an \texttt{unused} suffix.
(Unused variable are printed here because we passed \texttt{trim=FALSE}
to \texttt{evimp}. Normally they are omitted from the print.)

% The \texttt{col} column gives the column indices of the variables
% in the \texttt{x} argument to \texttt{earth}
% (after factors, if any, have been expanded; none in this example).

The \texttt{nsubsets} column is the number of subsets that included
the corresponding variable.  For example, \texttt{temp} appears in 10
subsets and \texttt{humidity} in 8.

The \texttt{gcv} and \texttt{rss} columns are scaled so
the largest net decrease is 100.

A ``\texttt{>}'' is printed after \texttt{gcv} and \texttt{rss}
entries that increase instead of decreasing (i.e., the ranking
disagrees with the \texttt{nsubsets} ranking).
We see that \texttt{ibh} is considered less important than
\texttt{dpg} using the \texttt{nsubsets} criterion, but not with the
\texttt{gcv} and \texttt{rss} criteria.

\subsection{Estimating variable importance in the MARS equation}
\label{sec-evimp-plotmo}

Running \texttt{plotmo} with \texttt{ylim=NULL} (the default)
gives an idea of which predictors in the MARS equation
make the largest changes to the predicted value
(but only with all other predictors at their median values).

Note that there is only a loose relationship between variable
importance in the MARS equation and variable importance in the data
(Section~\ref{sec-intro-evimp}).

% One can question if it is valid
% to estimate variable importance using model subsets that are not part of the final model.
% It is even possible for a variable to be rated as important yet not appear in the final model.

\subsection{Using \texttt{drop1} to estimate variable importance}

As an alternative to \texttt{evimp},
we can use the \texttt{drop1} function
(assuming we are using the formula interface to \texttt{earth}).
Calling \texttt{drop1(my.earth.model)} will delete each predictor in turn from the model,
rebuild the model from scratch each time, and calculate the GCV each time.
We will get warnings that the \texttt{earth} library function \texttt{extractAIC.earth} is
returning GCVs instead of AICs --- but that is what we want so we can
ignore the warnings.
(Turn off just those warnings by passing \texttt{warn=FALSE} to \texttt{drop1}.)
The column labeled \texttt{AIC} in the printed response
from \texttt{drop1} will actually be a column of GCVs not AICs.
The \texttt{Df} column isn't much use in this context.

Remember that this technique only tells us how important
a variable is with the other variables already in the model.
It does not tell us the effect of a variable in isolation.

We will get lots of output from \texttt{drop1} if we built the original \texttt{earth}
model with \texttt{trace>0}.
We can set \texttt{trace=0} by updating the model before calling \texttt{drop1}.
Do it like this: \texttt{my.model <- update(my.model, trace=0)}.

\subsection{Estimating variable importance by building many models}
\label{sec-building-many}

The variance of the variable importances estimated from an \texttt{earth} model
can be high (meaning that the estimates of variable importance in a
model built with a different realization of the data would be
different).

This variance can be partially averaged out by building a bagged
\texttt{earth} model and take the mean of the variable importances in
the many \texttt{earth} models that make up the bagged model.  You
can do this easily using the functions \texttt{bagEarth} and
\texttt{varImp} in Max Kuhn's \texttt{caret} package~\cite{caretpackage}.

Measuring variable importance using Random Forests is another way to go,
independently of \texttt{earth}.
See the functions \texttt{randomForest} and
\texttt{importance} in the
\texttt{randomForest} package.

\subsection{Remarks on \texttt{evimp}}

The \texttt{evimp} function is useful in practice but the following
issues can make it misleading.

Collinear (or otherwise related) variables can mask each other's importance,
just as in linear models.
This means that if two predictors are closely related, the forward pass will
somewhat arbitrarily choose one over the other.
The chosen predictor will incorrectly appear more important.

For interaction terms, each variable gets credit for the entire term ---
thus interaction terms are counted more than once
and get a total higher weighting than additive terms (questionably).
Each variable gets equal credit in interaction terms even though
one variable in that term may be far more important than the other.

MARS models can sometimes have a high variance --- if the data change
a little, the set of basis terms created by the forward pass can
change a lot.  So estimates of predictor importance can be unreliable
because they can vary with different training data.

For factor predictors, importances are estimated on a per-level basis
(because \texttt{earth} splits factors into indicator columns,
essentially treating each level as a separate variable).  The
\texttt{evimp} function should have an option to aggregate the
importances over all levels, but that has not yet been implemented.

TODO Enhance \texttt{evimp} to use cross-validation statistics when available.

\clearpage
\section{Cross-validating \texttt{earth} models}
\label{sec-cross-validating}

Use cross-validation to get an estimate of R-Squared on independent data.\newline
Example (note the \texttt{nfold} parameter):
\small
\begin{verbatim}
    > fit <- earth(survived ~ ., data=etitanic, degree=2, nfold=10)
    > summary(fit)

    Call: earth(formula=survived~., data=etitanic, nfold=10, degree=2)

        ... usual stuff not shown ...

    GCV 0.14  RSS 140  GRSq 0.422  RSq 0.444  cv.rsq 0.42

    Note: the cross-validation sd's below are standard deviations across folds

    Cross validation:   nterms 8.50 sd 0.53    nvars 5.40 sd 0.52

        cv.rsq    sd     ClassRate    sd     MaxErr  sd
          0.42 0.082          0.80 0.028       -1.3 1.1
\end{verbatim}
\normalsize
Cross-validation is done if \texttt{nfold} is greater than 1 (typically 5 or 10).
Earth first builds a standard model with all the data as usual.
This means that the standard fields in \texttt{earth}'s return value
appear as usual, and will be displayed as usual by \texttt{summary.earth}.
Earth then builds \texttt{nfold} cross-validated models.
For each fold it builds an \texttt{earth} model with the in-fold data
(typically nine tenths of the complete data) and using this model
measures the R-Squared from predictions made on the out-of-fold data
(typically one tenth of the complete data).
% \footnote[1]{We
% use ``in-fold'' to mean the training data;
% some authors have it the other way round.}
The final mean \texttt{cv.rsq} printed by \texttt{summary.earth}
is the mean of these out-of-fold R-Squared's.
% Use \texttt{summary.earth} to see this final value and its standard deviation
% across cross-validation folds.

The cross-validation results go into extra fields in \texttt{earth}'s return value.
All of these have a \texttt{cv} prefix ---
see the \texttt{Value} section of the \texttt{earth} help page for details.
For reproducibility, call \texttt{set.seed} before calling
\texttt{earth} with \texttt{nfold}.

% See Section~\ref{sec-nfolds2} for further discussion.

\subsection{What is the best value for \texttt{nfold}?}
\label{sec-nfolds1}

The question of choosing the number of cross-validation folds remains
in general an open research question.  We can only suggest that you
try 5- or 10-fold cross-validation, unless you have a small data set.
% There are studies that suggest 5 or 10 fold is often appropriate.

With a small data set some experimentation may be needed to get
results without too much variance.
A smaller \texttt{nfold} like 2 may be appropriate, otherwise the
out-of-fold test sets are so small that variance becomes too large.
(This assumes that the model is stable enough to be built on a small
training set, Section~\ref{sec-cv-bias}.)
We can average out the variance with \texttt{ncross=10} (say) but even
that is unstable if the out-of-fold sets are too small.
(Section~\ref{sec-ncross} discusses \texttt{ncross}.)

% Our inability to quantify the trade-offs makes it impossible to
% formulate clear rules for choosing \texttt{nfold}.
% Generally we want the test (out-of-fold) sets to be as large as
% possible (a small \texttt{nfold}).
% Some of the trade-offs for choosing the size of the in-fold sets are
% as follows.  We want the fold models to represent the population of
% full models.  This implies that the fold models should be
% \emph{similar} in form to the full model (e.g. a similar number of MARS
% terms, if that is what we would see in the population of full models).
% % We don't want a fold model that we would never see in the population.
% So we want the fold data sets to be big (a big \texttt{nfold}).
% Conversely, we want the fold models to \emph{differ} among
% themselves to adequately represent the diversity in the population of
% full models.  So we want the fold data sets to be small (a small
% \texttt{nfold}).

% However, where possible we want to avoid a too small \texttt{nfold},
% because we don't want training-set-size effects to kick in.  We want
% the in-fold sets to be large enough for the fold models to have a
% chance at being similar in form to the full model (about the same
% number of terms, for example).  Otherwise the cross-validation
% estimates will be biased (Section~\ref{sec-cv-bias}).

% On the other hand, a very large \texttt{nfold} (in the limit,
% leave-one-out) makes the in-fold training sets almost identical to
% each other.
% The fold models will identical to the full model.  Therefore we will
% not average out the kind of variation we would see if we actually used
% fresh training sets.  The resulting cross-validation estimates (such
% as the mean \texttt{cv.rsq}) will have a high variance --- meaning
% that they would tend to change a lot in the hypothetical situation
% where we got a fresh data set and repeated the cross-validation
% again.\footnote[1]{It is easy to confuse the variance of the mean
% \texttt{cv.rsq} \emph{across training sets} with the variance of the
% \texttt{cv.rsq} measured at each fold \emph{across folds}.  These
% actually have an inverse relationship once you have discounted
% variation introduced by measuring error on the limited-size
% out-of-fold sets.}

% In summary: \texttt{nfold} large means large variance and low bias;
% \texttt{nfold} low means low variance but high bias.  But reality is
% often not as clear cut as that.

\subsection{Using cross-validation to select the number of terms}
\label{using-cv-to-select}

Earth displays cross-validation statistics but currently does
not have facilities to automatically select
a model using the cross-validation results.
The following example shows how to do that with a bit of R code.

First we run the cross validation.  Then we rebuild the model with the
optimum number of terms determined by the cross validation.

\small
\vspace{-2mm}
\begin{verbatim}
library(earth)
library(bootstrap) # just for the "scor" data
set.seed(2) # for fold reproducibility, not strictly necessary
data(scor)
data <- data.frame(y=scor[,3], # didactic canonical data frame
                   x1=scor[,1], x2=scor[,2], x3=scor[,4], x4=scor[,5])

# Build an earth model with cross validation.
# Note that keepxy=TRUE to create detailed out-of-fold data.

mod <- earth(y~., data=data, nfold=5, keepxy=TRUE)
plot(mod, which=1, col.rsq=0, caption="Cross Validated Models")

# Select the number of terms that gives the maximum mean out-of-fold
# R-Squared.  This the number of terms shown by the vertical dotted red
# line in the graph, and criterion (ii) in the next section.  (There are
# other approaches for choosing the cross-validated number of terms.)

print(mod$cv.oof.rsq.tab, digits=2) # out-of-fold R-Squareds for every model size

mean.oof.rsq.per.subset <- mod$cv.oof.rsq.tab[nrow(mod$cv.oof.rsq.tab),]

nterms.selected.by.cv <- which.max(mean.oof.rsq.per.subset)

cat("\nnterms selected by GCV (standard earth model):", length(mod$selected.terms),
    "\nnterms selected by CV:                        ", nterms.selected.by.cv, "\n")

# Rebuild the earth model with the desired number of terms (and using
# all the data).  The penalty=-1 tells earth's backward pass to ignore
# the GCV.  This overrides the usual selection-by-min-GCV, which could
# return a smaller model than the given nprune.

mod.cv <- earth(y~., data=data, nprune=nterms.selected.by.cv, penalty=-1)

\end{verbatim}
\vspace{-10mm}
\normalsize

\subsection{Two ways of collecting R-Squared}
\label{sec-two-kinds}

Earth uses two ways of collecting the R-Squareds generated during cross-validation
(Figure~\ref{figs/cv-detailed}).
The names we use for these, \texttt{cv.rsq} and \texttt{oof.rsq}, are
somewhat arbitrary, but used consistently in \texttt{earth}'s code and
documentation.
\vspace{-3mm}
\begin{enumerate}[(i)]
\item
A folds's \texttt{cv.rsq} is calculated from predictions made on the
out-of-fold observations using the model built from the in-fold data.
Earth builds the fold model as usual, using the best
GCV of the training (in-fold) data.
The mean of these per-fold \texttt{cv.rsq}'s is the \texttt{cv.rsq}
printed by \texttt{summary.earth}.
\item
The \texttt{oof.rsq}'s are primarily for model selection,
i.e., for selecting the best number of terms.
They aren't printed by \texttt{summary.earth}, but by default are plotted
by \texttt{plot.earth} (Figure~\ref{figs/cv}).

\begin{SCfigure}
  \begin{centering}
  \includegraphics[width=.55\textwidth, clip, trim=0mm 134mm 132mm 0mm, keepaspectratio]{figs/cv-detailed.pdf} % trim l b r t
  \caption{\textit{Five-fold cross-validation of an} \texttt{earth} model.
\newline
\newline
 \textit{The pink lines are the} out-of-fold R-Squareds (\texttt{oof.rsq}) \textit{for each fold.
The thick red line is the mean of the pink lines.
\newline
\newline
A black dot shows the} \texttt{cv.rsq} \textit{for a fold.
It is the out-of-fold R-Squared for the selected model
at the fold. The} \texttt{cv.rsq} \textit{printed by} \texttt{summary.earth}
\textit{is the mean vertical position of these dots.
\newline
\newline
  }
}
  \label{figs/cv-detailed}
  \end{centering}
\end{SCfigure}

\begin{SCfigure}
  \begin{centering}
  \includegraphics[width=.56\textwidth, clip, trim=0mm 90mm 87mm 0mm, keepaspectratio]{figs/cv.pdf} % trim l b r t
  \caption{\textit{Default plot of five-fold cross-validation of an \texttt{earth} model.
\newline
\newline
 \textit{Same model as the figure above, but displaying slightly different information.}
  }
\newline
\newline
\newline
\newline
\newline
\newline
}
  \label{figs/cv}
  \end{centering}
\end{SCfigure}

A \texttt{oof.rsq} is calculated for every model size in each fold.
(The model size is the number of terms in the model.)
For each fold, it is calculated from predictions made on the
out-of-fold observations using a model built with the in-fold data,
after the model is pruned to the desired size.

% (At each fold the \texttt{oof.rsq} of the model with the highest GCV
% on the training data is that fold's \texttt{cv.rsq}.)

% In detail, the procedure is as follows.
% For each fold, a model is first built on the in-fold training data.
% The for each number of terms, that model is pruned to the number of terms.
% The pruned model i sused to make predictions on the out-of-fold data,
% yielding an \texttt{oof.rsq}.

Note: The \texttt{oof.rsq}'s are calculated only when \texttt{keepxy=TRUE},
because calculating them is slow --- \texttt{earth} has to call
\texttt{update.earth} and \texttt{predict} for every model size in every fold.
\end{enumerate}

\subsection{Cross-validation statistics returned by \texttt{earth}}
\label{cv-stats}

The previous section described the R-Squareds collected during cross-validation.
This section describes various
additional cross-validation statistics.

Each of these is measured on the out-of-fold set for each fold, and
summarized by averaging across all folds (except that \texttt{MaxErr}
is ``summarized'' by taking the worst error across all folds).
Use \texttt{summary.earth} to see the summary statistics and their standard
deviation across folds.\footnote[1]{We emphasize
that \texttt{summary.earth} prints the deviation \emph{across folds},
not the unknown deviation across hypothetical samples.
See Section~\ref{sec-cv-variance} ``Variance of cross-validation estimates''.
It is easy to confuse the two.}
See the \texttt{Value} section of the \texttt{earth} page for more
details of these statistics.

The statistics are:
\vspace{-3mm}
\begin{itemize}
\item{\texttt{cv.rsq}}{
See Section~\ref{sec-two-kinds} Part~(i).
% Calculated for each fold using the model built from the in-fold
% data, where the model is selected as usual using the in-fold GCV.
}
\item{\texttt{oof.rsq}}{
See Section~\ref{sec-two-kinds} Part~(ii).
% Calculated for all model sizes in each fold.
% Not printed by \texttt{summary.earth}.
}
\item{\texttt{MaxErr}}{ Signed max absolute difference between the
predicted and observed response.
This is the maximum of the absolute differences, multiplied by
\texttt{-1} if the sign of the difference is negative.
The ``summary'' \texttt{MaxErr} is the worst \texttt{MaxErr} across folds.
}

\item{\texttt{ClassRate}}{ (discrete responses only) Fraction of out-of-fold
observations correctly classified.
For binary responses a decision threshold is \texttt{0.5} is used.
}
\end{itemize}
\vspace{-3mm}
If we cross-validate a binomial or poisson model
(specified using \texttt{earth}'s \texttt{glm} argument),
\texttt{earth} returns the following additional statistics:
% Note that the returned \texttt{cv.rsq} is the same as for standard
% (non-glm) \texttt{earth} models.
\vspace{-3mm}
\begin{itemize}
\item{\texttt{MeanDev}}{ Deviation divided by the number of observations.}

\item{\texttt{CalibInt}, \texttt{CalibSlope}}{
Calibration intercept and slope (from regressing the observed response
on the predicted response).}

\item{\texttt{AUC}}{ (Binomial models only) Area under the ROC curve.}

\item{\texttt{cor}}{ (Poisson models only) Correlation between the predicted and observed response.}
\end{itemize}
\vspace{-3mm}
For multiple response models, at each fold \texttt{earth} calculates these statistics for each
response independently, and combines them by taking their mean,
or weighted mean if the \texttt{wp} argument is used.
Taking the mean is a rather dubious way of combining results from
what are essentially quite different models,
but can nevertheless be useful.

Explanations of the above GLM statistics can be found in the following
(and other) references: Pearce and Ferrier~\cite{Pearce},
Fawcett~\cite{Fawcett}, and Harrell~\cite{Harrell}.
See the source code in \texttt{earth.cv.lib.R} for details of how
the statistics are calculated, based on code kindly made available by Jane
Elith and John Leathwick.
\vspace{3mm}

TODO Allow the user to select the model size which gives the best mean
out-of-fold classification rate (need a \texttt{cv.oof.classrate.tab}
table like \texttt{cv.oof.rsq.tab}).

\subsection{Tracing cross-validation}

With \texttt{trace=.5} or higher,
\texttt{earth} prints progress information as cross-validation proceeds.
For example
\vspace{-4mm}
\small
\begin{verbatim}
    CV fold 3: cv.rsq 0.622  n.oof 86 12%  n.fold.nz 384 41%  n.oof.nz 43 39%
\end{verbatim}
\normalsize
\vspace{-3mm}
shows that in cross-validation fold 3, the \texttt{cv.rsq} for the
fold model was 0.622, measured on the 86 observations in the
out-of-fold set.

The print also shows the number and percentage of non-zero values
in the observed response in the in-fold and out-of-fold sets.
This is useful if we have a binary or factor response and want to check
that we have enough examples of each factor level in each fold.
With the \texttt{stratify} argument (which is enabled by default), \texttt{earth}
attempts to keep the numbers of occurrences of any given level in the
response constant across folds.

\subsection{Plotting cross-validation results}
\label{sec-plot-cv}

If you want \texttt{plot.earth} to show cross-validation statistics,
use \texttt{keepxy=TRUE} so \texttt{earth} calculates the
\texttt{oof.rsq} for every model size in every fold.  Example
(Figure~\ref{figs/cv} on page~\pageref{figs/cv}),
further discussion in Section~\ref{sec-cv-and-plot}:
\vspace{-4mm}
\begin{verbatim}
    fit <- earth(survived ~ ., data=etitanic, degree=2,
                 nfold=5, keepxy=TRUE, trace=.5)
    plot(fit, which=1) # which=1 selects just the Model Selection plot
\end{verbatim}
\vspace{-3mm}
In the figure, the pink lines for some of the fold models are truncated at the right.
This is because the maximum number of terms for those models happens to be
less than the 16 terms in the full model.
% For more details, see Chapter~\ref{sec-cross-validating} ``Cross-validation''.

\begin{SCfigure}[][b]
  \begin{centering}
  \includegraphics[width=.5\textwidth, clip, trim=0mm 120mm 110mm 0mm, keepaspectratio]{figs/cv-folds.pdf} % trim l b r t
  \caption{\textit{Comparing the GCV curves of models at three cross-validation folds.
  }
\newline
\newline
\newline
\newline
\newline
}
  \label{figs/cv-folds}
  \end{centering}
\end{SCfigure}

For the curious, \texttt{plot.earth.models} can be used to compare the
GCV curve of models built at each fold. Example
(Figure~\ref{figs/cv-folds}):
\vspace{-4mm}
\begin{verbatim}
    fit <- earth(survived ~ ., data=etitanic, degree=2,
                 nfold=3, keepxy=TRUE)
    plot.earth.models(fit$cv.list, which=1, ylim=c(0, .5))
\end{verbatim}
\vspace{-3mm}

% TODO
% For multiple response models, at each fold \texttt{earth} calculates the RSq for each
% response independently, and combines these by taking their mean
% (or weighted mean if the \texttt{wp} argument is used).

\subsection{The \texttt{ncross} argument}
\label{sec-ncross}

If we run \texttt{earth} twice with the same \texttt{nfold} argument
we will get different cross-validation results,
because \texttt{earth} randomly splits the data into folds differently each time.
% (Call \texttt{set.seed} before calling \texttt{earth} to get repeatable results.)
To average out this variation for more stable results,
use the \texttt{ncross} argument
to repeat the whole process of taking \texttt{nfold} folds multiple times.
Example (Figure~\ref{figs/ncross}):
\vspace{-4mm}
\begin{verbatim}
    fit <- earth(survived ~ ., data=etitanic, degree=2,
                 ncross=3, nfold=5, keepxy=T, trace=.5)
    plot(fit, which=1, col.rsq=0)
\end{verbatim}

TODO What are the statistical properties of \texttt{ncross}?

\begin{SCfigure}
  \begin{centering}
  \includegraphics[width=.5\textwidth, clip, trim=0mm 90mm 80mm 0mm, keepaspectratio]{figs/ncross.pdf} % trim l b r t
  \caption{\textit{Cross-validating with} \texttt{ncross=3, nfold=5}.\newline
There are 15 folds in all.
\newline
\newline
\newline
\newline
\newline
\newline
\newline
\newline
\newline
  }
  \label{figs/ncross}
  \end{centering}
\end{SCfigure}

\subsection{An example: training versus generalization error}
\label{cv-example}

Figure~\ref{figs/esl-fig71} on the next page is reproduced from Figure~7.1 in Hastie
et al.~\cite{ESL}.
The figure was created from models built with 100 training sets
generated synthetically.

Figure~\ref{figs/cv-example} is an example
illustrating some of the same principles.
Instead of using new data, we use cross-validation.
(Also, we use \texttt{earth} and the \texttt{mtcars} data.)
The figure was created with the following code:
\vspace{-4mm}
\begin{verbatim}
    fit <- earth(mpg~., data=mtcars, ncross=10, nfold=2, keepxy=TRUE)
    plot(fit, which=1,
         col.mean.infold.rsq="blue", col.infold.rsq="lightblue",
         col.grsq=0, col.rsq=0, col.vline=0, col.oof.vline=0)
\end{verbatim}
\vspace{-3mm}

\begin{SCfigure}
  \begin{centering}
  \includegraphics[width=.6\textwidth, clip, trim=0mm 0mm 0mm 0mm, keepaspectratio]{figs/esl-fig71-lores.jpg} % trim l b r t
  \caption{\textit{Reproduced from Figure~7.1 of
Hastie et al.~\cite{ESL}
\newline
\newline
For models built from a 100 training sets,
the pale blue lines show the prediction error measured on the training set.
The pink lines show the error measured on a very large independent
test set.
\newline
\newline
The thick lines show the expected error for the corresponding set of pale lines.
\newline
  }}
  \label{figs/esl-fig71}
  \end{centering}
\end{SCfigure}

\begin{SCfigure}
  \begin{centering}
  \includegraphics[width=.6\textwidth, clip, trim=0mm 94mm 88mm 6mm, keepaspectratio]{figs/cv-example.pdf} % trim l b r t
  \caption{\textit{Cross-validation of an \texttt{earth} model on the
\texttt{mtcars} data.
\newline
\newline
The Model Complexity along the horizontal axis in
the figure above becomes the Number of Terms in this figure.
\newline
\newline
Note the negative R-Squareds in the pink curves at the far left of the graph
(FAQ~\ref{faq05}).
\newline
\newline
\newline
}}
  \label{figs/cv-example}
  \end{centering}
\end{SCfigure}

Figure~\ref{figs/cv-example} is ``upside down'' with respect to
Figure~\ref{figs/esl-fig71} because it plots model performance, not
lack-of-performance.  But we still see the same basic structure: the
performance measured on the training data increases as we increase
model complexity; on independent data the performance peaks and then
decreases.
% The more the model overfits the training data the more it
% ``underfits'' the test data.

Many of the pale lines in Figure~\ref{figs/cv-example} are truncated
because the maximum number of
terms generated by \texttt{earth} for those fold models is less than
the 9 terms in the full model.
We mention also that several of the
arguments in the call to \texttt{plot.earth} above simply remove
display elements.  The defaults for these arguments are inappropriate
for this somewhat unusual plot (we are not usually interested in the
in-fold R-Squareds).

Much of the variation of the pink curves in Figure~\ref{figs/cv-example}
is due to the
relatively small size of the out-of-fold data sets.  If we measured
R-Squared on a very large test set (instead of the out-of-fold data)
we would still see variation, but much less.
But note how variation of the pink lines increases with the number of
terms.
The more flexible the model, the more it overfits to randomness in
the training set, and thus more randomness enters its estimation of
R-Squared on independent data.

The \texttt{mtcars} data set is small (32 observations).
Only two folds were used above (but repeated 10 times with
\texttt{ncross}) to keep the out-of-fold sets large enough for
somewhat stable results.
With this small \texttt{nfold}, cross-validation bias may be an issue,
because of the small size of the in-fold sets relative to the full
data set.
So the R-Squared on the out-of-fold data will tend to be smaller than
it would be across full-sized independent samples.
See Section~\ref{sec-cv-bias} ``Bias of cross-validation estimates''.

% (Section~\ref{sec-cv-variance} ``Variance of
% cross-validation estimates'').

The maximum mean out-of-fold RSq is at 3 terms, which in this example
coincides with the number of terms selected by the GCV of the full
model (set \texttt{col.grsq} to see this, not shown).
A larger number of terms would have been selected with \texttt{degree}
equal to 2, which is actually more appropriate for the \texttt{mtcars} data.

\clearpage
\section{Understanding cross-validation}
\label{sec-understanding-cv}

This chapter tries to clarify some aspects of cross-validation.
It was written in response to email about
cross-validating \texttt{earth} models.
The chapter is mostly a general discussion, not limited to \texttt{earth} models.
The exposition takes a frequentist approach, using arguments based on
hypothetical situations where we have access to extra data.

% A summary of this chapter is:

% (i) the expected value of cross-validation R-Squared matches the
% expected prediction error across training sets (up to bias,
% Section~\ref{sec-learning-curve}).

% (ii) the variance of cross-validation R-Squared cannot be used to
% estimate the variance of prediction R-Squared across training sets.

We assume that you already know the basics of cross-validation
(i.e., partition the data into \texttt{nfold} subsets, repeatedly build a
model on all but one of those subsets, measure performance on the
left-out data).

For a description of cross-validation, see for example
Hastie et al.~\cite{ESL}, Section 7.10,
Duda et al.~\cite{Duda00} Section 9.6, or
even Wikipedia\newline \url{http://en.wikipedia.org/wiki/Cross-validation_(statistics)}.\newline
An in-depth reference is Arlot and Celisse~\cite{Arlot}.

\subsection{Data sets for measuring performance}
\label{sec-data-sets}

In the next section we will discuss what it is that cross-validation
actually measures.
But first in this section we review some aspects of measuring a
model's performance, and the data sets needed to do that.
Understanding the role of these data sets is important for applying
cross-validation correctly
(Section~\ref{sec-cv-mistakes} ``Common cross-validation mistakes'').

% This review applies to all kinds of regression model, not just
% \texttt{earth} models.

Typically we want to measure our model's \emph{generalization} performance,
and so want to measure prediction error on new data, i.e., not on the
training data.  (If that isn't immediately obvious, please see FAQ~\ref{faq07}.)
We typically want to measure
performance in two scenarios:
\vspace{-4mm}
\begin{enumerate}[(i)]
\item For {\bf parameter selection}, i.e., to choose certain key model parameters
during the model building process.  For example, for \texttt{earth} models we
need to select the best number of terms
(so the parameter here is the number of terms).
And for \texttt{rpart} trees we need to choose the optimum tree size.
The new data are used as \emph{parameter-selection data},
also commonly called \emph{model-selection} or \emph{validation} data.
\item For {\bf model assessment}, i.e., to measure the performance
of the final model.  Here the new data are used as \emph{test data}.
\end{enumerate}
\vspace{-3mm}
We thus require three independent data sets:
\vspace{-4mm}
\begin{enumerate}[(i)]
\item the {\bf training} data,\vspace{-3mm}
\item the {\bf parameter-selection} data for (i) above,\vspace{-3mm}
\item the {\bf test} data for (ii) above.
\end{enumerate}
\vspace{-3mm}
The ideal way to meet these requirements is to actually have large
amounts of new data drawn from the same population.
Usually we don't actually have access to such data,
and so must resort to other techniques.

One such technique is the GCV used when building MARS models, which
bypasses the need for model-selection data by using a formula to approximate
the RSS that would be measured on new data.

Another technique, more universal, is cross-validation.
Depending on how it is used, cross-validation can emulate either the
parameter-selection or test data.

Some readers may wonder why we don't bother with the above data sets
for linear models.
When building a linear model, there is no separate model-selection
step, so we don't need a model-selection set.
And with these simple models the difference between the
residual sum-of-squares measured on the training data and on
independent test data is inconsequential, provided certain assumptions
are met.
So we don't need a separate test set.
Things change with more a flexible modeling procedure (for example, if we are doing
automatic variable selection for a linear model).
With a more flexible model overfitting becomes more likely.
The difference between the residual
sum-of-squares on the training data and on independent test data
becomes consequential, and formulas to estimate prediction error get
complicated, if they exist at all.
% See also FAQ~\ref{faq07}.

\subsection{What does cross-validation measure?}

The mechanics of cross-validation are easy to understand.
Understanding what cross-validation actually estimates involves some
subtleties.

\textbf{Cross-validation estimates ``expected'' not ``conditional'' error}

Cross-validation does not really estimate the generalization
performance of our model, built on a single set of data (which
is usually what we want to know when applying a modeling
technique like \texttt{earth}).
Instead, cross-validation estimates the performance of our \emph{model
building algorithm} on a \emph{range of training sets}.
It approximates the average performance we would see in a hypothetical
scenario where we build many models, each on a fresh sample of training
data of approximately the same size as the original sample, and measure
the performance of each of those models on independent test data
(all data being i.i.d. from the same population).

In other words, cross-validation is better at estimating the expected
prediction error across training sets, not the prediction error
conditional on the training data we have at hand.
In Figure~\ref{figs/esl-fig71} on page~\pageref{figs/esl-fig71}, our
model is one pink line but cross-validation approximates the solid red
line.
See also Hastie et al.~\cite{ESL} Section ~7.12 ``Conditional or Expected Test Error?''

Some details.  Cross-validation differs from the hypothetical scenario
above because in cross-validation the training (in-fold) sets share
observations and are not as varied as they would be in the
hypothetical scenario.  Also, the training set of a fold incorporates
test sets from other folds.  This induces a relationship between the
residual errors (or whatever is used to measure performance) across
folds, particularly in the presence of outliers.
% In cross-validation the size of the training set and test sets are
% very constrained, determined by the number of observations and
% \texttt{nfold}.

\textbf{Expected value of R-Squared across models}

Consider the hypothetical scenario above, and for concreteness let us
measure performance as R-Squared on the independent test data.
If we built many models (with training sets of constant size) and took
the average R-Squared over all the models, we would eventually close
in on a stable average R-Squared value.
This average R-Squared would be the same regardless of the size of the
test sets, assuming we repeated the experiment enough times
(all data being i.i.d. from the same population).
In other words, the \emph{expected value} of R-Squared across models
does not depend on the size of the test sets --- but the
\emph{variance} of the R-Squared's certainly does, which leads to the
next section.

\textbf{Variance of R-Squared across models}

Once we have an estimate of the generalization performance of the
model (such as R-Squared on independent data), we typically want to know the
\emph{stability} of that estimate,
usually expressed as the variance of the estimate.

Typically we want to know how our estimated R-Squared would be
expected to change if we had a different training set --- the sampling
variance of R-Squared.
% \footnote{We gloss over quibbles about the
% appropriateness of using variance as a measure of dispersion for a
% statistic that clamps at~1.}
% (with R-Squared being measured over the population of test data).
This is the variance we would measure across models in the
hypothetical scenario above if the test sets were extremely large
(so all variation is due to the training sets, not the test sets).

On the other hand, if the test sets are small, the variance of R-Squared
will include extra variation due to the small size of the test sets.
And this is the scenario emulated by cross-validation.
In cross-validation, it isn't possible in general to disentangle the
variability due to the in-fold training sets and the variability due
to the small out-of-fold test sets.
\begin{SCfigure}[][b]
  \begin{centering}
  \includegraphics[width=.5\textwidth, clip, trim=2mm 90mm 114mm 0mm, keepaspectratio]{figs/learning-curve.pdf} % trim l b r t
  \caption{\textit{The estimated learning curve of an \texttt{earth} model.
\newline
\newline
The dark line is the mean of the gray lines.
\newline
\newline
Each gray line shows the GRSq for one set of subsets of the
training data.
  }
\newline
\newline
\newline
\newline
\newline
\newline
}
  \label{figs/learning-curve}
  \end{centering}
\end{SCfigure}
\subsection{Bias of cross-validation estimates}
\label{sec-cv-bias}

Cross-validation tries to establish the quality of a model built on
the full sample by using models built on \emph{smaller} subsets of the
sample (often 4/5 or 9/10 of the sample).
Generally a model built with a subset will be ``worse'' than a model
built with the full sample.
Thus the cross-validation R-Squared\footnote[1]{The
\emph{Cross-validation R-Squared} here is the mean of the
R-Squareds of each fold, each measured on the out-of-fold data.
It is the \texttt{cv.rsq} printed by \texttt{summary.earth}.}
will tend to be lower than it should be.
That is, the cross-validation R-Squared is conservatively \emph{biased}.

To see where the model sits on the learning curve (Figure~7.8 in
Hastie et al.~\cite{ESL}), one technique for \texttt{earth} models is
plot the GRSq of models built with different sized subsets of the
sample, and average out variation by repeating several times.
We make the assumption that the behavior of the model's GRSq in the
face of a smaller sample is an adequate indication of that behavior
for the full model's R-Squared (measured on independent data).
Figure~\ref{figs/learning-curve} gives an example,
produced by the following R code.
(You can ignore the code and just look at the figure.)
\vspace{-4mm}\small\begin{verbatim}
    learning.curve <- function(data, func, field="grsq", ncurves=30)
    {
        # set up the plot (call func on full data to establish ylim)
        body <- body(func) # needed only for the plot title
        plot(0, xlim=c(0,1), ylim=c(0, 1.2 * func(data)[[field]]), type="n",
             xlab="fraction of data", ylab=field, cex.main=1.1, xpd=NA,
             main=paste("estimated learning curve\n",
               substr(paste(deparse(substitute(body)), collapse=""), 1, 40)))
        grid(col="linen", lty=1)
        all.results <- rep(0, 10)
        for(curve in 1:ncurves) {                      # for each gray line
            sample <- sample.int(nrow(data))
            results <- double(10)
            for(fold in 1:10) {
                sub.data <- data[sample[1:(fold * nrow(data) / 10)],]
                results[fold] <- func(sub.data)[[field]]
            }
            lines((1:10)/10, results, col="gray")      # one gray line
            all.results <- all.results + results
        }
        lines((1:10)/10, all.results / ncurves, lwd=2) # the mean line
    }

    learning.curve(etitanic,
                   function(data) earth(pclass~., data, degree=2))
\end{verbatim}
\normalsize

From the figure, the bias is small enough with 10 folds
(the curve is flat enough with 90\% of the data).
% Even though the curve is not flat at 90\%, the bias is likely smaller
% or of the same order as the unknown variance.

Remember that this is just an \emph{estimated} learning curve because
it is created from a single training sample.
Ideally we would like to estimate the learning curve using many
fresh training sets.
If we did that, we would see variation in the curves on the far right
of graph which we do not see in Figure~\ref{figs/learning-curve}.  So
even with \texttt{nfold=5} the bias is acceptably small, relative to
the (unknown) variance.

% We mention that the \texttt{learning.curve} function above
% can be used on other models.
% Example (not shown):
% \vspace{-6mm}
% \begin{verbatim}
%     learning.curve(etitanic,
%                    function(data) summary(lm(survived~., data)),
%                    field="r.squared")
% \end{verbatim}

\subsection{Variance of cross-validation estimates}
\label{sec-cv-variance}

% The variance of test R-Squared across training sets is something we
% usually want to know.\footnote[1]{The
% variance we are referring to is the variation in R-Squared that
% would occur if we measured R-Squared on an extremely large test set
% across models built from different training sets (drawn from the same
% population and of the same size as the original training set).}
% Without new data, I know of no way of estimating this variance for MARS models.

Cross-validation bias seems to be much discussed, but usually a more
serious problem with cross-validation is the \emph{variance} of
cross-validation estimates across samples, and our inability to
estimate this variance.
How much would we expect the cross-validation R-Squared (averaged
across folds) to change if we used a different training sample?
(The new sample would be of the same size and drawn from the same
population as the original, and, to make things more confusing,
perhaps even generate the same model, but usually not.)
It is not much comfort to know that the \emph{expected} value of a
statistic is correct up to small bias, if the single statistic we have
at hand could be far from that expected value.

Quantifying the variance of cross-validation statistics in general is
an ongoing research problem (see Bengio and
Grandvalet~\cite{bengio04} for a clear explanation of
some of the difficulties involved).
Unfortunately it is not really possible to estimate the variance of
the cross-validation R-Squared of \texttt{earth} models.

An indication is given by the variance of the CV R-Squared across
folds (printed by \texttt{summary.earth} as a standard deviation).
However, this variance includes extra variability because we are looking
at the R-Squared per fold instead of the mean R-Squared across folds.
On the other hand, it incorporates less variability due to training sets than
if we actually used fresh training data at each fold.
Also it is unstable because of the small size of the out-of-fold test
sets (the variance of the variance is high).

For \texttt{earth} models, another indication is the variance of
GRSq in the estimated learning curve (Figure~\ref{figs/learning-curve}).
Assuming GRSq is an acceptable surrogate for R-Squared on independent
data, the variance across the gray curves gives an
approximate lower bound of R-Squared variance across models for
variously sized training sets.
We say ``lower bound'' because the estimated learning curve is created
from only a single training set (if we used fresh data for each model
the variance at the right of the curve would not taper to zero).

% The variance of cross-validation R-Squared across cross-validation
% folds will typically be much greater because it includes extra
% variation caused by the small out-of-fold sets.

% \subsection{What is the best value for \texttt{nfold}, revisited}
% \label{sec-nfolds2}

% This section continues the discussion in Section~\ref{sec-nfolds1}.

\subsection{Common cross-validation mistakes}
\label{sec-cv-mistakes}

In this section we list some mistakes that are easy to make when
cross-validating.
All of these make the model's performance seem better than it is.
Given how easy it is to make these mistakes, a certain amount of
skepticism is warranted when papers present final model assessment
statistics based on cross-validation.

The central point is that \emph{any data that have been used to select
model parameters cannot be used as independent test data}.
This rule can be violated in subtle ways, as discussed below.

\vspace{2mm}
{\bf Independence of observations}

The out-of-fold data must play the role of new data.  It is thus
important that it isn't ``contaminated'' by the in-fold training data.
This implies that the observations must be independent.  Lack of
independence means that the in-fold data used for training are
partially included in some sense in the out-of-fold data used for
testing. Thus cross-validation will tend to give an optimistic
R-Squared and select an overfitted model.  At a minimum, we should
avoid ``twinned'' observations.

\vspace{2mm}
{\bf Pre-tuning}

Cross-validation must be applied to the entire model building process.
Any parameter that is tuned to the training data must not be tuned
before cross-validation begins.
Instead, it must be included in the cross-validation process.
(This does not apply to decisions made independently of the training data.)
For example, it is a mistake to use the training data to choose which
subset of the variables to include in the model (before calling \texttt{earth}),
then cross-validate the \texttt{earth} model (with \texttt{nfold}) using only that
subset of variables.\footnote[1]{The ``parameter'' being tuned here is
which predictors are good.}
The convenience of \texttt{earth}'s \texttt{nfold} argument makes it perhaps a
little too easy to make that kind of mistake.
% (This same principle applies to the t-values of the coefficients of a
% linear model, which will be too high if linear regression is done
% using a subset of variables selected beforehand by examination of the
% data.)

A word of explanation for the above paragraph.
Let's say we do in fact optimize a parameter to the full data set before
cross-validation begins.
By optimizing to the full data, we are also to some extent optimizing
to the out-of-fold data used during cross-validation
(because the out-of-fold data are, after all, drawn from the full data).
The out-of-fold data are thus contaminated and can no longer
legitimately play the role of independent test data, and
the R-Squared's measured on the out-of-fold data will tend to be
better than they should be.

There are however a few contexts where it is acceptable to select
variables before cross-validation.  See the comments in Hastie et
al..~\cite{ESL} Section 7.10.2.

% Trevor says the following is wrong.
% However, if we are using cross-validation for model \emph{selection}
% (Scenario (i) in Section~\ref{sec-data-sets}), then it is often
% acceptable to tune parameters before doing cross-validation.  This is
% because for model selection we are interested only in the
% \emph{relative} performance of models.
% We have to make the assumption that pre-tuning does not change the
% relative order of the R-Squareds of the models undergoing
% selection (even though it affects their absolute values).
% A slightly dangerous assumption, but often plausible enough
% for pragmatic model selection.
% % (Indeed, the use of GCVs for model selection makes this assumption.
% % The penalty used when calculating GCVs is not adjusted upward for
% % variables that are pre-selected before running \texttt{earth}.)
% (Hastie et al..~\cite{ESL} Section 7.10.2 takes a harder line,
% disallowing pre-tuning even if only estimating parameters,
% but perhaps because their discussion is in the context of
% pre-screening predictors where there are a small number of
% observations and a very much larger number of predictors.)

\vspace{2mm}
{\bf Conflating the validation and test data}

If the cross-validation R-Squared is used to select a model then the
test R-Squared quoted for the selected model must be
\emph{recalculated} for that model using independent data.  The
cross-validation R-Squared used to select a model cannot be quoted as
the R-Squared of that model --- that would be conflating the
validation and test data.

% \clearpage
% \begin{figure}[b]
%   \begin{center}
%   \includegraphics[width=1.06\textwidth, clip, trim=2mm 122mm 2mm 0mm, keepaspectratio]{figs/plotd-example2.pdf} % trim l b r t
%   \caption{\texttt{plotd} \textit{examples}}
%   \label{figs/plotd-example2}
%   \end{center}
% \end{figure}
%
% \section{The \texttt{plotd} function}
%
% The \texttt{plotd} function plots the distribution of the predicted values
% for each class using a density plot or histogram.
% Example (Figure~\ref{figs-plotd-example2}):
% \vspace{-4mm}
% \begin{verbatim}
%     fit <- earth(survived ~ ., data=etitanic, degree=2)
%     plotd(fit)
%     plotd(fit, vline.col=1, err.col=c(2,3,4))
%     plotd(fit, hist=TRUE, type="class", labels=TRUE, xlab="", xaxis.cex=.8)
% \end{verbatim}
% \vspace{-3mm}
% The function was designed for \texttt{earth} models, but
% can also be used on models built by
% \texttt{lm},
% \texttt{glm},
% \texttt{lda},
% etc.
%
% This function calls \texttt{predict} with the data originally
% used to build the model, and with the \texttt{type} specified by the user.
% It then separates the predicted values into classes, where the class
% for each predicted value is determined by the class of the observed
% response.
% Finally, it calls \texttt{density} (or \texttt{hist} if
% \texttt{hist=TRUE}) for each class-specific set of values, and plots the
% results.
%
% This function estimates distributions with the
% \texttt{density} and \texttt{hist} functions,
% and also calls \texttt{plot.density} and \texttt{plot.histogram}.
% For an overview of those functions see Venables and Ripley MASS~\cite{Venables02} Section 5.6.
%
% \subsection{Partitioning the response into classes}
%
% Considerable effort is made to partition the predicted response into
% classes in a sensible way.
%
% This is not always possible for multiple column responses and the
% \texttt{nresponse} argument should be used where necessary to select the
% column of the predicted response.
% The partitioning details depend on the types and numbers of columns in
% the observed and predicted responses.
% These in turn depend on the model object and the \texttt{type} argument.
% Use the \texttt{trace} argument to see how \texttt{plotd} partitions the
% response for your model.
%
% \begin{SCfigure}
%   \includegraphics[width=.4\textwidth, clip, trim=2mm 100mm 100mm 0mm, keepaspectratio]{figs/plotd-glm.pdf} % trim l b r t
%   \caption{\texttt{plotd} \textit{with} \texttt{glm}
% \newline
% \newline
% \newline
% }
%   \label{figs/plotd-glm}
% \end{SCfigure}
%
% \subsection{Using \texttt{plotd} with various models}
%
% This function is included in the \texttt{earth} package
% but can also be used with other models.
%
% Example with \texttt{glm} (Figure~\ref{figs/plotd-glm}):
% \vspace{-4mm}
% \begin{verbatim}
%     glm.model <- glm(sex ~ ., data=etitanic, family=binomial)
%     plotd(glm.model)
% \end{verbatim}
% \vspace{-3mm}
% Example with \texttt{lm} (not shown):
% \vspace{-4mm}
% \begin{verbatim}
%     lm.model <- lm(as.numeric(sex) ~ ., data=etitanic)
%     plotd(lm.model)
% \end{verbatim}
%
% \subsection{Using \texttt{plotd} with \texttt{lda} and \texttt{qda}}
%
% The \texttt{plotd} function has special handling for \texttt{lda}
% (and \texttt{qda}) objects in the \texttt{MASS} package.
% For such objects, the \texttt{type} argument can take one of the
% following values:
% \vspace{-4mm}
% \begin{itemize}
% \renewcommand{\labelitemi}{$\cdot$}
% \item \texttt{"response"} (default) linear discriminant.\vspace{-3mm}
% \item \texttt{"ld"} same as \texttt{"response"}.\vspace{-3mm}
% \item \texttt{"class"} predicted classes.\vspace{-3mm}
% \item \texttt{"posterior"} posterior probabilities.
% \end{itemize}
% \vspace{-3mm}
%
% Example:
% \vspace{-4mm}
% \begin{verbatim}
%     library(MASS)
%     lda.model <- lda(sex ~ ., data=etitanic)
%     plotd(lda.model) # linear discriminant by default
%     plotd(lda.model, type="class", hist=TRUE, labels=TRUE)
% \end{verbatim}
%
% This handling of \texttt{type} is handled internally by \texttt{plotd}
% and \texttt{type} is not passed to \texttt{predict.lda}
% (\texttt{type} is used merely to select fields in the list
% returned by \texttt{predict.lda}).
% The type names can be abbreviated down to a single character.
%
% For objects created with \texttt{lda.matrix}
% (as opposed to \texttt{lda.formula}),
% \texttt{plotd} blindly assumes that the \texttt{grouping} argument was the second argument.
%
% \texttt{plotd} does not yet support objects created with \texttt{lda.data.frame}.
%
% % Example with \texttt{rpart}:
% % \vspace{-4mm}
% % \begin{verbatim}
% %        library(rpart); library(earth); data(etitanic)
% %        rpart.model <- rpart(sex ~ ., data = etitanic, method="class")
% %        plotd(rpart.model, type="prob", nresponse=1)
% %        plotd(rpart.model, type="prob", nresponse=2)
% %        plotd(rpart.model, type="class", hist=TRUE, labels=TRUE)
% % \end{verbatim}
% \begin{SCfigure}
%   \begin{centering}
%   \includegraphics[width=.8\textwidth, clip, trim=2mm 100mm 2mm 0mm, keepaspectratio]{figs/plotd-lda.pdf} % trim l b r t
%   \caption{\texttt{plotd} \textit{with} \texttt{lda}
% \newline
% \newline
% \newline
% }
%   \label{figs/plotd-lda}
%   \end{centering}
% \end{SCfigure}
% For \texttt{lda} responses with more than two factor levels,
% use the \texttt{nresponse} argument to
% select a column in the predicted response.
% Thus with the default \texttt{type=NULL},
% (which gets automatically converted by \texttt{plotd} to \texttt{type="response"}),
% use \texttt{nresponse=1} to select just the first linear discriminant.
% The default \texttt{nresponse=NULL} selects all columns,
% which is typically not what you want for \texttt{lda} models.
% Example (not shown):
% \vspace{-4mm}
% \begin{verbatim}
%     library(MASS)
%     example(lda)     # creates a model called "z"
%     plot(z, dimen=1) # invokes plot.lda from the MASS package
%     plotd(z, nresponse=1, hist=1) # equivalent using plotd
%                                   # nresponse=1 selects first linear discr.
% \end{verbatim}
%
% The \texttt{dichot=TRUE} argument is also useful for \texttt{lda}
% responses with more than two factor levels.

\clearpage
\section{FAQ}
\label{faq}

\subsection{What are your plans for \texttt{earth}?}
\label{faq00}

We would like to add support of case weights (to allow boosting),
but that won't happen anytime soon.

We would also like to add the capability to pass \texttt{earth} an
existing model (a linear, MARS, or basically any model) and
\texttt{earth} would build a MARS model that fills in the remaining
residuals.

Proper support of NAs would be good too.

\subsection{How do I cite the \texttt{earth} package?}
\label{faq19}

Thank you for asking that question :)

For \texttt{earth} the following BibTex entry seems to do the trick.
The extra curly braces in the author field are necessary to get
BibTex to order the entry correctly on the last name of the first author.
\vspace{-4mm}
\begin{verbatim}
	@book{earthpackage,
	  title  = {earth: Multivariate Adaptive Regression Spline Models},
	  author = {S. {Milborrow. Derived from mda:mars by T. Hastie and R. Tibshirani.}},
	  publisher={R Software Package},
	  year   = {2011},
	  note   = {\url{http://www.milbo.users.sonic.net/earth}}
	}
\end{verbatim}
\vspace{-3mm}
From within R you can use (you will have to massage the results
to get BibTex to order the entry correctly):
\vspace{-4mm}
\begin{verbatim}
    > library(earth)
    > citation("earth")
\end{verbatim}
% \vspace{-2mm}
% To cite this document itself, use
% \vspace{-4mm}
% \begin{verbatim}
%     @Manual{earthnotes,
%       title = {Notes on the \texttt{earth} package},
%       author = {Stephen Milborrow},
%       year = {2011},
%       note = {\url{www.milbo.org/doc/earth-notes.pdf}}
%     }
% \end{verbatim}

\subsection{How can I establish variable importance?}
\label{faq01}

Use the \texttt{evimp} function.
See its help page and Chapter~\ref{evimp} for more details.

The \texttt{summary.earth} function lists the predictors
in order of estimated importance
using the \texttt{nsubsets} criterion of \texttt{evimp}.

\subsection{Which predictors are used in the model?}
\label{faq02}

The following function will give a list of predictors in the model:
\vspace{-4mm}
\begin{verbatim}
get.used.pred.names <- function(obj) # obj is an earth object
{
  any1 <- function(x) any(x != 0)    # like any but no warning if x is double
  names(which(apply(obj$dirs[obj$selected.terms, , drop=FALSE], 2, any1)))
}
\end{verbatim}

\subsection{Which predictors were added to the model first?}
\label{faq03}

You can see the forward pass adding terms with \texttt{trace=2} or higher.
But remember, pruning will usually remove some of the terms.
You can also use
\vspace{-4mm}
\begin{verbatim}
    summary(my.model, decomp="none")
\end{verbatim}
\vspace{-3mm}
which will list the terms remaining after pruning,
in the order they were added by the forward pass.
But it should be remarked that the order in which terms or predictors are added is
not necessarily indicative of their importance.

\subsection{Can I use \texttt{earth} with a binary response?}
\label{faq04a}

Yes.  Typically you want to predict a probability.
Usually the best way to proceed is:

1) Convert your binary response to a \texttt{logical}, if
it is not that already.  To do the conversion, you can use code like this:
\vspace{-5mm}
\begin{verbatim}
    df$response <- df$response == "survived"
\end{verbatim}
\vspace{-4mm}
Actually, you don't really have to do that.  Although a response of
type \texttt{logical} is canonical in this situation,
if you prefer you could also use a two-level factor or a 1s-and-0s response.
Earth doesn't mind.

2) Include \texttt{glm=list(family=binomial))} in your call to \texttt{earth}.
For an example, see Section~\ref{glmexamples}(i).
This builds a model that estimates the probability of a ``true'' response.

See Chapter~\ref{sec-glm} for examples and details.

\subsection{Can I use \texttt{earth} with a categorical response?}
\label{faq04b}

Yes.  If your response has only two categories (it's binary)
please see the above FAQ.

If your response has more than two levels, make sure your response is
an unordered R \texttt{factor}.
For an example, see Section~\ref{glmexamples}(ii),
and the rest of that chapter for details.

Before handing it to its internal engine,
\texttt{earth} will expand your reponse to multiple columns
(an indicator columm for each factor level) and
build a multiple-response model.
This is described in Chapter~\ref{sec-factors}.

\subsection{How can I train on one set of data and test on another?}
\label{faq04}

The example below demonstrates one way to train on 80\% of the data and
test on the remaining 20\%.
\vspace{-4mm}
\begin{verbatim}
    train.subset <- sample(1:nrow(trees), .8 * nrow(trees))
    test.subset <- (1:nrow(trees))[-train.subset]
    fit <- earth(Volume ~ ., data = trees[train.subset, ])
    yhat <- predict(fit, newdata = trees[test.subset, ])
    y <- trees$Volume[test.subset]
    print(1 - sum((y - yhat)^2) / sum((y - mean(y))^2)) # print R-Squared
\end{verbatim}
\vspace{-3mm}
In practice a data set larger than the one in the example should be used for splitting.
The model variance is too high with this small set --- run the example a few
times to see how the model changes as \texttt{sample}
splits the data set differently on each run.
Also, remember that the test set should not be used for parameter tuning
because you will be optimizing for the test set ---
instead use GCVs, separate parameter-selection sets, or techniques
such as cross-validation with \texttt{earth}'s \texttt{nfold} parameter.
(Cross-validation repeats the process in the above
code five or ten times, using a different subset each time,
Sections~\ref{sec-cross-validating} and~\ref{sec-understanding-cv}.)

\subsection{What is a GCV, in simple terms?}
\label{faq06}

GCVs are important for MARS because the pruning pass
uses GCVs to evaluate model subsets.

In general terms, when testing a model (not necessarily a MARS model)
we want to test \emph{generalization} performance, and so want to
measure error on independent data, i.e., not on the training data.
Often a decent set of independent data is unavailable and so we
resort to cross-validation or leave-one-out methods.
But that introduces other complications and can be painfully slow.
As an alternative, for certain forms of model we can use a formula to
approximate the error that would be determined by leave-one-out
validation --- that approximation is the GCV.
The formula adjusts (i.e., increases) the training RSS to take into
account the flexibility of the model.
Summarizing, the GCV approximates the RSS (divided by the number of cases)
that would be measured on independent data.
Even when the approximation is not that good,
it is usually good enough for comparing models during pruning.

GCVs were introduced by Craven and Wahba~\cite{Craven}, and
extended by Friedman and Silverman~\cite{FriedmanSilverman,Friedman}.
See Hastie et al.~\cite{ESL}, Section 7.10 ``Cross-Validation'',
and the Friedman MARS paper~\cite{Friedman}.
GCV stands for ``Generalized Cross Validation'', a perhaps misleading term.
because no cross-validation is actually performed.
% For example, the terms selected by the pruning pass for each subset size
% are the same whether the GCV or RSS is used to select terms.
% Actual cross-validation during pruning would choose terms for each subset that
% are different in general from those selected by the RSS on the full training set.

The GRSq measure used in the \texttt{earth} package standardizes the raw GCV,
in the same way that R-Squared standardizes the RSS
(FAQ~\ref{faq05a}).

\subsection{If GCVs are so important, why don't linear models use them?}
\label{faq07}

First a few words about overfitting.
An overfit model fits the training data well but will not give good predictions on new data.
The idea is that the training data capture the underlying structure
in the system being modeled, plus noise.
We want to model the underlying structure and ignore the noise.
An overfit model models the specific realization of noise in the training data
and is thus too specific to that training data.

The more flexible a model, the more its propensity to overfit the training data.
Linear models are constrained, with usually only a few parameters
(viz.\ the intercept and regression coefficients)
and don't have the tendency to overfit like more flexible models such as MARS.
This means that for linear models, the RSS on the data used to build
the model is usually an adequate measure of generalization ability,
and we don't need GCVs.

This is no longer true if we do automatic variable selection on linear models,
because the process of selecting variables increases the flexibility
of the model. Hence the AIC --- as used in, say, \texttt{drop1}.
The GCV, AIC, and friends are means to the same end.
Depending on what information is available during model building.
we use one of these statistics to estimate model generalization performance
for the purpose of selecting a model.

\subsection{Can R-Squared be negative?}
\label{faq05}

Yes, R-Squared (\texttt{rsq}) can be negative if
\vspace{-4mm}
\begin{enumerate}[(i)]
\item the test set is not the training set (for example, in cross-validation), and,

\item we use the general definition of R-Squared
\vspace{-3mm}
\begin{verbatim}
        rsq = 1 - rss / tss,
\end{verbatim}
\vspace{-3mm}
where \texttt{rss = sum((y - yhat)\textasciicircum 2)} is the residual sum-of-squares
and \texttt{tss =  sum((y - mean(y)\textasciicircum 2)} is the total sum-of-squares.
This is the definition used in the \texttt{earth} code.
\end{enumerate}
\vspace{-2mm}

% If we calculate the residual sum-of-squares \texttt{rss} on new data, it
% is quite possible for \texttt{rss} to be greater than \texttt{tss}.
% In that situation \texttt{rsq = 1 - rss / tss} will be negative.

The simplest example is an intercept-only model.
An intercept-only model always predicts a constant value, the mean of
the training data.
An intercept-only model will give a negative \texttt{rsq} on test
data, unless the training data and test data happen to have the same
mean. Why is this?
On the test data, the intercept-only model predicts, as always, the
mean of the \emph{training} data.
Thus on the test data the residuals will be greater on the whole than
if we predicted the mean of the test data.\footnote[1]{Recall
that $\sum_i(x_i - \mu)^2$ is minimimized
when $\mu$ is the mean of the $x_i$'s.}
That is another way of saying that on the test data
the residual sum-of-squares
will be greater than the total sum-of-squares,
and \texttt{rsq = 1 - rss / tss} will be negative.
Examples can be seen in the left of Figure~\ref{figs/cv-example}
on page~\pageref{figs/cv-example}.

There is actually more than way of defining \texttt{rsq}.
You may be more familiar with the definition
\vspace{-4mm}
\begin{verbatim}
        rsq = regression.sum.of.squares / tss,
\end{verbatim}
\vspace{-3mm}
which is indeed always non-negative.
With \texttt{rsq} measured on the training data,
the two definitions are equivalent for linear regression with an
intercept and for \texttt{earth} models.
The Wikipedia page on RSq has a clear explanation (accessed May 2011)
\url{http://en.wikipedia.org/wiki/R-squared}.

The ``squared'' in R-Squared is misleading.  Perhaps we should use the
alternative term ``coefficient of determination'', but ``R-Squared''
is common.

\subsection{Can GRSq be negative?}
\label{faq05a}

Yes.
The statistic \texttt{GRSq} is \texttt{earth}'s estimate of the generalization performance
of the model.
It is defined, analogously to R-Squared (FAQ~\ref{faq05}), as
\vspace{-4mm}
\begin{verbatim}
    GRSq = 1 - GCV / GCV.null,
\end{verbatim}
\vspace{-3mm}
where \texttt{GCV.null} is the GCV of an intercept-only model.

A negative \texttt{GRSq} indicates a severely over parameterized model ---
a model that would not generalize well even though it may be a good fit
to the training data.
During \texttt{earth} model building, \texttt{GRSq} can become negative.
However, after pruning the model will end up with a non-negative \texttt{GRSq}.

Adding a term to the model will always increase the R-Squared
on the training data (up to the limits of numerical accuracy).
But adding that term could reduce the predictive power of the model on new data,
and would thus \emph{decrease} \texttt{GRSq}.
(We see that happening in almost any \texttt{earth} Model Selection graph.)
Decrease \texttt{GRSq} often enough and it will eventually become negative.
Watch the \texttt{GRSq} take a nose dive in this
example~(Figure~\ref{figs/negative-grsq}):
\begin{verbatim}
    fit <- earth(mpg~., data=mtcars, trace=4)
    plot(fit, which=1, col.npreds=0,
         col.sel.grid="linen", legend.pos="bottomleft")
\end{verbatim}
\vspace{-3mm}
In severe cases, GRSq might even be set to \texttt{-Inf}, which brings
us to the following FAQ.

\begin{SCfigure}
  \begin{centering}
  \includegraphics[width=.6\textwidth, clip, trim=0mm 90mm 80mm 12mm, keepaspectratio]{figs/negative-grsq.pdf} % trim l b r t
  \caption{\textit{Negative GRSq's.
\newline
\newline
After term 3, adding a term reduces the estimated predictive
power of the model, apart from term 7.
By term 14 the GRSq is negative.
\newline
\newline
The R-Squared on the training data always increases as \texttt{earth} adds terms.
\newline
\newline
  }
}
  \label{figs/negative-grsq}
  \end{centering}
\end{SCfigure}

\subsection{Why am I seeing a GRSq of \texttt{-Inf} (with \texttt{trace} enabled)?}
\label{faq05b}

During the forward pass, if ``too many'' terms are generated relative
to the number of observations, \texttt{earth} will set the model's GCV
to \texttt{Inf} and consequentially the GRSq to \texttt{-Inf}.
However, after pruning the final model's GCV and GRSq will be non-negative
(so you will only see a negative GRSq if tracing is enabled).

Infinite GCVs were introduced in \texttt{earth} Version~3.1-2, to
replace the warning issued in previous versions (which is no longer needed):
\vspace{-4mm}
\begin{verbatim}
    effective number of GCV parameters >= number of cases.
\end{verbatim}
\vspace{-3mm}

Some details.
Earth sets the GCV to \texttt{Inf} during model building if the
effective number of parameters for a term is greater than the number
of observations.
The GCV no longer approximates the leave-one-out RSS.
To see this, consider the formula for the GCV
\vspace{-4mm}
\begin{verbatim}
    GCV = RSS / (nobs * (1 - nparams / nobs)^2)).
\end{verbatim}
\vspace{-3mm}
From the formula we see that the GCV increases and then decreases if
\texttt{nparams / nobs}
approaches and then exceeds~1 as terms are added to the model.
To prevent this undesirable non-monotonic behavior, if \texttt{nparams / nobs >= 1} then
\texttt{earth} does not use the formula but instead directly sets the
GCV to \texttt{Inf}.

% TODO removed until I can establish if and how Gillian did the
% case weighting necessary for bootstrapping \texttt{earth} models.
%
% \subsection{What about boosting MARS?}
% \label{faq08}
%
% If you want to boost, use boosted trees rather than boosted MARS --- you
% will get better results.
%
% More precisely, although gradient boosted MARS gives
% better results than plain MARS,
% if you would like to improve prediction performance (at the cost
% of a more complicated and less interpretable model)
% you will usually get better results with
% boosted trees (via, say, the \texttt{gbm} package) than with boosted MARS.
% See Gillian Ward (2007) \emph{Statistics in Ecological Modeling:
% Presence-Only Data and Boosted Mars (Doctoral Thesis)}
% \url{www-stat.stanford.edu/~hastie/THESES/Gill\_Ward.pdf}.
%
% This could change as the state of the art advances.

\subsection{How is the default number of terms \texttt{nk} calculated?}
\label{faq9a}

If we don't explicitly specify \texttt{nk}, the default is used:
\vspace{-4mm}
\begin{verbatim}
    nk = min(200, max(20, 2 * ncol(x))) + 1
\end{verbatim}
\vspace{-3mm}
This doubles the number of predictors,
forces that into the range of 20 to 200, and
finally adds 1 for the intercept.

The numbers 20 and 200 are fairly arbitrary.
The lower limit of 20 seems reasonable for situations where
we have just a few predictors.
The upper limit of 200 prevents excessive memory use in the forward
pass.  Typically we will reach one of termination conditions
long before we reach 200 terms.
(The termination conditions are described in
Section~\ref{sec-termination} ``Termination conditions for the forward pass''.)

\subsection{Why do I get fewer terms than my specified \texttt{nk}, even\newline
with \texttt{pmethod="none"}?}
\label{faq10}

There are several conditions that can terminate the forward pass,
and reaching \texttt{nk} is just one of them.
See Section~\ref{sec-termination} ``Termination conditions for the forward pass''.
The various stopping conditions mean that the actual number of terms
created by the forward pass may be less than \texttt{nk}.

There are other reasons why the actual number of terms may be less
than \texttt{nk}:

(i) The forward pass discards one side of a term pair
if it adds nothing to the model ---
but the forward pass counts terms as if they were actually created in pairs.
To see this behaviour, run earth with \texttt{trace=2} or higher.
You will see in the \texttt{Terms} column that although earth usually adds two terms
it will sometimes add just one.

(ii) As a final step (just before the backward pass), 
the forward pass deletes linearly dependent terms, if any,
so all terms in \texttt{dirs} and \texttt{cuts} are independent.
If this happens you will get a message like
\texttt{Fixed rank deficient bx by removing 3 terms}.

And remember that the pruning pass will usually discard further terms
unless \texttt{pmethod="none"}.

\subsection{Why do I get fewer terms than my specified \texttt{nprune}?}
\label{faq11}

The pruning pass selects a model with the lowest GCV
that has \texttt{nprune} or fewer terms.
Thus the \texttt{nprune} argument specifies the \emph{maximum}
number of permissible terms in the final pruned model.

You can work around this to get exactly \texttt{nprune} terms
by specifying \texttt{penalty=-1}.
An example:
\vspace{-4mm}
\begin{verbatim}
    earth(Volume ~ ., data=trees, trace=3, nprune=3, penalty=-1)
\end{verbatim}
\vspace{-4mm}
(This special value of \texttt{penalty} causes \texttt{earth} to set the
GCV to \texttt{RSS/nrow(x)}.
Since the RSS on the training set always decreases with more terms,
the pruning pass will choose the maximum allowable number of terms.)

\subsection{Is it best to hold down model size with \texttt{nk} or \texttt{nprune}?}
\label{faq12}

If you want the best possible small model, build a big set of basis
functions in the forward pass (by specifying a big \texttt{nk})
and prune this set back (by specifying a small \texttt{nprune}).
This is better than directly building a small model by
specifying a small \texttt{nk}.
You will get a better set of terms because the pruning pass can look
at all the terms whereas the forward pass can only see one term ahead.
However, it is faster building a small model by specifying a small
\texttt{nk}.

\subsection{What about bagging MARS?}
\label{faq09}

The \texttt{caret} package~\cite{caretpackage} provides functions for bagging
\texttt{earth} (and for parameter selection).  Our personal experience
has been that bagging MARS does not give models with better predictive
ability (probably because the MARS algorithm is fairly stable in the
presence of perturbations of the data, and bagging works best for
``unstable'' models).
Your mileage may vary (we would be interested if it does).
We tested just a couple of data sets, but did try a few different
approaches, including using a modified version of \texttt{earth} that
randomized the set of variables available at each forward step to
increase variability (similar to random forests).

\subsection{Why do I get
\texttt{Warning:\,glm.fit:\,fitted probabilities\newline
numerically 0 or 1 occurred?}}
\label{faq13}

You will only see this warning when using \texttt{earth}'s \texttt{glm}
argument.
You can safely ignore the warning in an \texttt{earth} context.
The GLM coefficients for the model
terms may be very large, but it doesn't matter --- the predictive
ability of the model is unimpaired.

The warning is issued when \texttt{glm.fit} generates a model
that perfectly separates the classes in the training data.
A perfect fit is usually considered a good thing, not something that
should cause a warning.
However, the warning is issued because certain model statistics (such
as the t-values) generated by the mathematics inside \texttt{glm.fit}
will be unreliable for subsequent inference on the model.
That doesn't matter in an \texttt{earth} context, because \texttt{earth}
doesn't use those statistics.
(And, anyway, the t-values are meaningless even when the warning is
not issued, because of the amount of processing done by \texttt{earth} to
generate the terms before it calls \texttt{glm.fit}.)

The warning message is more likely to occur during cross-validation
(using \texttt{earth}'s \texttt{nfold} parameter).
With cross-validation we are looking at more, and smaller, data sets,
so the chance of a perfectly separable set is more likely.
If the warning is issued, the coefficients for the terms of the fold
model may be very large, but they aren't used in the final model anyway.
The cross-validation statistics calculated by \texttt{earth} (such as
\texttt{cv.rsq}) remain valid.

\subsection{Why do I get \texttt{Error:\,XHAUST returned error code -999}?}
\label{faq14}

The short answer: you should never see the above message (fixed in
\texttt{earth} version 2.6-0).
If you do, please let us know.

One work-around is to change \texttt{pmethod} from
\texttt{"exhaustive"} to \texttt{"backward"}.

You can also try the following.
These instructions work on the assumption that the default value of
\texttt{Exhaustive.tol} is too big for your data set.
First please read the description of the \texttt{Exhaustive.tol} argument
in the \texttt{Arguments} section of the \texttt{earth} help page.
Then run \texttt{earth} with \texttt{trace=1}, so \texttt{earth} prints the
reciprocal of the condition number of the \texttt{earth} basis matrix
\texttt{bx}.  (The condition number here is the ratio of largest to the
smallest singular value of \texttt{bx}.)
Then set \texttt{Exhaustive.tol} to greater than the printed value (something like
\texttt{Exhaustive.tol=1e-8}), and run \texttt{earth} again.
Now \texttt{earth} will automatically change \texttt{pmethod} from
\texttt{"exhaustive"} to \texttt{"backward"} when necessary to avoid the
above error message.

It must be said that it is hard to believe under these conditions
that the resulting model will be much good.  The data do not allow a
decent predictive model to be built.

Some details.  Certain data cause collinearity in the
\texttt{earth} basis matrix \texttt{bx} which slips by the usual checks.
This causes the \texttt{leaps} routine to fail.
The usual checks are:

(i) while building the basis matrix, the C code does a check to drop
collinear terms (\texttt{BX\_TOL} and \texttt{QR\_TOL} in the C code)

(ii) after building the basis matrix, the C code drops any remaining
collinear terms (\texttt{RegressAndFix} in the C code)

(iii) the \texttt{leaps} Fortran routine \texttt{sing} checks for
collinearity.

Some data get through all these tests, probably because we are near
the numerical noise floor and numerical rounding is essentially
changing the data randomly.
When \texttt{pmethod="exhaustive"}, \texttt{earth}
performs an SVD of \texttt{bx}, and as a last resort
if the condition number is out-of-range
forces \texttt{pmethod} from \texttt{"exhaustive"} to \texttt{"backward"}.

\subsection{How does \texttt{summary.earth} order terms?}
\label{faq15}

With \texttt{decomp="none"},
the terms are ordered as created by the forward pass.

With the default \texttt{decomp="anova"},
the terms are ordered in increasing order of interaction.
In detail:
\vspace{-4mm}
\begin{enumerate}[(i)]
\item terms are sorted first on degree of interaction\vspace{-3mm}
\item then terms with a \texttt{linpreds} linear factor before standard terms\vspace{-3mm}
\item then on the predictors (in the order of the columns in the input matrix)\vspace{-3mm}
\item and finally on increasing knot values.
\end{enumerate}
\vspace{-2mm}
It's actually \texttt{earth:::reorder.earth} that does the ordering.

\subsection{Why is \texttt{plot.earth} not showing the cross-validation data?}
\label{faq15a}

Use \texttt{keepxy=TRUE} in the call to \texttt{earth} (as well as \texttt{nfold}).
See Section~\ref{sec-plot-cv}.

% \subsection{Why are the out-of-fold lines in \texttt{plot.earth} ``short''?}
% \label{faq15b}

% Earth generated less terms for (some of) the fold models than for the
% full model (Section~\ref{sec-plot-cv}).

\subsection{How do I add a plot to an existing page with \texttt{plot.earth} or \texttt{plotmo}?}
\label{faq20}

Use \texttt{do.par = FALSE}, otherwise these plotting functions start a new page.

\subsection{\texttt{summary.earth} lists predictors with weird names that aren't in \texttt{x}.  What gives?}
\label{faq16}

You probably have factors in your \texttt{x} matrix,
and \texttt{earth} is applying \texttt{contrasts}.
See Chapter~\ref{sec-factors} ``Factors''.

\subsection{What happened to \texttt{get.nterms.per.degree},\newline
\texttt{get.nused.preds.per.subset}, and \texttt{reorder.earth}?}
\label{faq18}

From release 1.3.0, some \texttt{earth} functions are no longer public,
to help simplify the user interface.
The functions are still available (and stable) if you need them ---
use for example \texttt{earth:::reorder.earth()}.

\clearpage
\bibliographystyle{plain}
\bibliography{earth-notes}
% \addcontentsline{toc}{chapter}{Bibliography}
\end{document}
